{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ef4cc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76f5151f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [\"--config\", \"train_config.yaml\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e288f71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liangchengwei/miniconda3/envs/test/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-09-02 14:59:50,136\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "import json\n",
    "import gc\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from datasets.distributed import split_dataset_by_node\n",
    "from tensordict import TensorDict\n",
    "from torchdata.stateful_dataloader import StatefulDataLoader\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "from areal.api.cli_args import (\n",
    "    GenerationHyperparameters,\n",
    "    GRPOConfig,\n",
    "    load_expr_config,\n",
    ")\n",
    "from areal.api.io_struct import (\n",
    "    FinetuneSpec,\n",
    "    ModelRequest,\n",
    "    WeightUpdateMeta,\n",
    ")\n",
    "from areal.api.workflow_api import RolloutWorkflow\n",
    "from areal.api.cli_args import GRPOConfig\n",
    "from areal.engine.ppo.actor import FSDPPPOActor\n",
    "from areal.engine.sglang_remote import RemoteSGLangEngine\n",
    "from areal.utils.data import concat_padded_tensors\n",
    "from areal.utils.device import log_gpu_stats\n",
    "from areal.utils.saver import Saver\n",
    "from areal.utils.stats_logger import StatsLogger\n",
    "from realhf.api.core.data_api import load_hf_tokenizer\n",
    "from realhf.base import logging, seeding, stats_tracker\n",
    "\n",
    "logger = logging.getLogger(\"TIR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0b74772",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AgentRLConfig(GRPOConfig):\n",
    "    max_tokens_per_traj: int = field(\n",
    "        default=32000,\n",
    "        metadata={\n",
    "            \"help\": \"maximum number of tokens per trajectory\"\n",
    "        }\n",
    "    )\n",
    "    max_tokens: int = field(\n",
    "        default=32000,\n",
    "        metadata={\n",
    "            \"help\": \"maximum number of tokens (including input and output) for the model\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    max_turns: int = field(\n",
    "        default=128,\n",
    "        metadata={\n",
    "            \"help\": \"maximum number of turns for search agent\"\n",
    "        }\n",
    "    )\n",
    "    n_trajs: int = field(\n",
    "        default=1,\n",
    "        metadata={\n",
    "            \"help\": \"We could collect multiple trajectories for a single query. By default n_trajs=1.\"\n",
    "        }\n",
    "    )\n",
    "    executor_url: str = field(\n",
    "        default=\"http://localhost:1451\",\n",
    "        metadata={\n",
    "            \"help\": \"URL of the code executor service\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    dump_dir: str = field(\n",
    "        default=\"./dump\",\n",
    "        metadata={\n",
    "            \"help\": \"directory to dump the trajectories\"\n",
    "        }\n",
    "    )\n",
    "    verbose: bool = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            \"help\": \"whether to print verbose information\"\n",
    "        }\n",
    "    )\n",
    "    recover_start_step: int = field(\n",
    "        default=0,\n",
    "        metadata={\n",
    "            \"help\": \"step to start recovering from, useful for resuming training\"\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abb861a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/liangchengwei/lcw/ZERO-TIR-RL/experiments/logs/liangchengwei/tir-grpo/trial0/generated'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hydra.core.global_hydra import GlobalHydra\n",
    "GlobalHydra.instance().clear()\n",
    "config, _ = load_expr_config(args, AgentRLConfig)\n",
    "config: AgentRLConfig\n",
    "\n",
    "\n",
    "config.dump_dir = os.path.join(\n",
    "    StatsLogger.get_log_path(config.stats_logger), \"generated\"\n",
    ")\n",
    "\n",
    "config.dump_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fd04bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from areal.utils.network import find_free_ports\n",
    "\n",
    "SGLANG_PORT, MASTER_PORT = 11451, 14514\n",
    "\n",
    "SGLANG_HOST = \"127.0.0.1\"\n",
    "\n",
    "# Environment variables used by inference/train engines\n",
    "import os\n",
    "\n",
    "os.environ[\"AREAL_LLM_SERVER_ADDRS\"] = f\"{SGLANG_HOST}:{SGLANG_PORT}\"\n",
    "os.environ[\"MASTER_ADDR\"] = \"127.0.0.1\"\n",
    "os.environ[\"MASTER_PORT\"] = str(MASTER_PORT)\n",
    "os.environ[\"RANK\"] = str(0)\n",
    "os.environ[\"WORLD_SIZE\"] = str(1)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "os.environ[\"LOCAL_RANK\"] = str(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1abc6321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.sglang.tokenizer_path = config.sglang.model_path\n",
    "config.sglang.model_path = \"/home/liangchengwei/lcw/ZERO-TIR-RL/experiments/checkpoints/liangchengwei/tir/debug/default/epoch0epochstep399globalstep399\"\n",
    "config.sglang.skip_tokenizer_init = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f31b06ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGLang服务器已启动，进程号(PID): 2373777\n",
      "SGLang服务器启动中，请等待初始化完成...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liangchengwei/miniconda3/envs/test/lib/python3.12/subprocess.py:1016: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\n",
      "/home/liangchengwei/miniconda3/envs/test/lib/python3.12/subprocess.py:1021: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stderr = io.open(errread, 'rb', bufsize)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STDERR] [2025-09-02 15:00:00] server_args=ServerArgs(model_path='/home/liangchengwei/lcw/ZERO-TIR-RL/experiments/checkpoints/liangchengwei/tir/debug/default/epoch0epochstep399globalstep399', tokenizer_path='/home/liangchengwei/lcw/ZERO-TIR-RL/experiments/checkpoints/liangchengwei/tir/debug/default/epoch0epochstep399globalstep399', tokenizer_mode='auto', skip_tokenizer_init=True, skip_server_warmup=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, dtype='bfloat16', kv_cache_dtype='auto', quantization=None, quantization_param_path=None, context_length=32768, device='cuda', served_model_name='/home/liangchengwei/lcw/ZERO-TIR-RL/experiments/checkpoints/liangchengwei/tir/debug/default/epoch0epochstep399globalstep399', chat_template=None, completion_template=None, is_embedding=False, enable_multimodal=None, revision=None, hybrid_kvcache_ratio=None, impl='auto', host='127.0.0.1', port=11451, nccl_port=None, mem_fraction_static=0.8, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=-1, max_prefill_tokens=32768, schedule_policy='lpm', schedule_conservativeness=1.0, cpu_offload_gb=0, page_size=1, tp_size=1, pp_size=1, max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=1, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=1, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http='warning', log_requests=False, log_requests_level=0, crash_dump_folder=None, show_time_cost=False, enable_metrics=True, bucket_time_to_first_token=None, bucket_e2e_request_latency=None, bucket_inter_token_latency=None, collect_tokens_histogram=False, decode_log_interval=10, enable_request_time_stats_logging=False, kv_events_config=None, api_key=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, dp_size=1, load_balance_method='round_robin', dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend='fa3', sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, ep_size=1, enable_ep_moe=False, enable_deepep_moe=False, enable_flashinfer_moe=False, enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, cuda_graph_max_bs=None, cuda_graph_bs=None, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_nccl_nvls=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, disable_overlap_schedule=False, disable_overlap_cg_plan=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_torch_compile=False, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through_selective', hicache_io_backend='', flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, enable_return_hidden_states=False, enable_triton_kernel_moe=False, warmups=None, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, num_reserved_decode_tokens=512, pdlb_url=None, custom_weight_loader=[], weight_loader_disable_mmap=False)\n",
      "[STDERR] [2025-09-02 15:00:07] Init torch distributed begin.\n",
      "[STDERR] [2025-09-02 15:00:07] Init torch distributed ends. mem usage=0.00 GB\n",
      "[STDERR] [2025-09-02 15:00:08] Load weight begin. avail mem=48.49 GB\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.75it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.75it/s]\n",
      "[STDERR] [2025-09-02 15:00:09] Load weight end. type=Qwen2ForCausalLM, dtype=torch.bfloat16, avail mem=45.42 GB, mem usage=3.07 GB.\n",
      "[STDERR] [2025-09-02 15:00:09] KV Cache is allocated. #tokens: 1337086, K size: 17.85 GB, V size: 17.85 GB\n",
      "[STDERR] [2025-09-02 15:00:09] Memory pool end. avail mem=9.12 GB\n",
      "[STDERR] [2025-09-02 15:00:09] Capture cuda graph begin. This can take up to several minutes. avail mem=9.10 GB\n",
      "[STDERR] [2025-09-02 15:00:09] Capture cuda graph bs [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160]\n",
      "Capturing batches (bs=1 avail_mem=7.70 GB): 100%|██████████| 23/23 [00:05<00:00,  4.07it/s] \n",
      "[STDERR] [2025-09-02 15:00:15] Capture cuda graph end. Time elapsed: 5.75 s. mem usage=1.41 GB. avail mem=7.69 GB.\n",
      "[STDERR] [2025-09-02 15:00:15] max_total_num_tokens=1337086, chunked_prefill_size=-1, max_prefill_tokens=32768, max_running_requests=4096, context_len=32768, available_gpu_mem=7.69 GB\n",
      "[STDERR] [2025-09-02 15:00:16] Prefill batch. #new-seq: 1, #new-token: 3, #cached-token: 0, #token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, timestamp: 2025-09-02T15:00:16.463947\n",
      "[STDERR] [2025-09-02 15:00:17] The server is fired up and ready to roll!\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import threading\n",
    "import time\n",
    "\n",
    "# 启动sglang server\n",
    "from areal.api.cli_args import SGLangConfig\n",
    "from areal.utils.network import find_free_ports\n",
    "\n",
    "config.sglang.log_level = \"info\"\n",
    "config.sglang.decode_log_interval = 10\n",
    "sglang_cmd = SGLangConfig.build_cmd(\n",
    "    config.sglang,\n",
    "    tp_size=1,\n",
    "    base_gpu_id=1,\n",
    "    host=SGLANG_HOST,\n",
    "    port=SGLANG_PORT,\n",
    ")\n",
    "\n",
    "def read_pipe(pipe, prefix):\n",
    "    \"\"\"实时读取管道输出并在notebook中显示\"\"\"\n",
    "    for line in iter(pipe.readline, b''):\n",
    "        try:\n",
    "            line_str = line.decode('utf-8').rstrip()\n",
    "            if line_str:\n",
    "                print(f\"[{prefix}] {line_str}\")\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"[{prefix}] <binary output>\")\n",
    "    pipe.close()\n",
    "\n",
    "sglang_process = subprocess.Popen(\n",
    "    sglang_cmd,\n",
    "    shell=True,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    bufsize=1,\n",
    "    universal_newlines=False\n",
    ")\n",
    "\n",
    "# 获取并打印进程号\n",
    "print(f\"SGLang服务器已启动，进程号(PID): {sglang_process.pid}\")\n",
    "\n",
    "# 启动线程实时读取stdout和stderr\n",
    "stdout_thread = threading.Thread(target=read_pipe, args=(sglang_process.stdout, \"STDOUT\"))\n",
    "stderr_thread = threading.Thread(target=read_pipe, args=(sglang_process.stderr, \"STDERR\"))\n",
    "stdout_thread.daemon = True\n",
    "stderr_thread.daemon = True\n",
    "stdout_thread.start()\n",
    "stderr_thread.start()\n",
    "\n",
    "print(\"SGLang服务器启动中，请等待初始化完成...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
