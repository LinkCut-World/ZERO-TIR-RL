{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ef4cc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76f5151f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [\"--config\", \"train_config.yaml\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e288f71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liangchengwei/miniconda3/envs/test/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-09-02 15:16:33,085\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "import json\n",
    "import gc\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from datasets.distributed import split_dataset_by_node\n",
    "from tensordict import TensorDict\n",
    "from torchdata.stateful_dataloader import StatefulDataLoader\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "from areal.api.cli_args import (\n",
    "    GenerationHyperparameters,\n",
    "    GRPOConfig,\n",
    "    load_expr_config,\n",
    ")\n",
    "from areal.api.io_struct import (\n",
    "    FinetuneSpec,\n",
    "    ModelRequest,\n",
    "    WeightUpdateMeta,\n",
    ")\n",
    "from areal.api.workflow_api import RolloutWorkflow\n",
    "from areal.api.cli_args import GRPOConfig\n",
    "from areal.engine.ppo.actor import FSDPPPOActor\n",
    "from areal.engine.sglang_remote import RemoteSGLangEngine\n",
    "from areal.utils.data import concat_padded_tensors\n",
    "from areal.utils.device import log_gpu_stats\n",
    "from areal.utils.saver import Saver\n",
    "from areal.utils.stats_logger import StatsLogger\n",
    "from realhf.api.core.data_api import load_hf_tokenizer\n",
    "from realhf.base import logging, seeding, stats_tracker\n",
    "\n",
    "logger = logging.getLogger(\"TIR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0b74772",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class AgentRLConfig(GRPOConfig):\n",
    "    max_tokens_per_traj: int = field(\n",
    "        default=32000,\n",
    "        metadata={\n",
    "            \"help\": \"maximum number of tokens per trajectory\"\n",
    "        }\n",
    "    )\n",
    "    max_tokens: int = field(\n",
    "        default=32000,\n",
    "        metadata={\n",
    "            \"help\": \"maximum number of tokens (including input and output) for the model\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    max_turns: int = field(\n",
    "        default=128,\n",
    "        metadata={\n",
    "            \"help\": \"maximum number of turns for search agent\"\n",
    "        }\n",
    "    )\n",
    "    n_trajs: int = field(\n",
    "        default=1,\n",
    "        metadata={\n",
    "            \"help\": \"We could collect multiple trajectories for a single query. By default n_trajs=1.\"\n",
    "        }\n",
    "    )\n",
    "    executor_url: str = field(\n",
    "        default=\"http://localhost:1451\",\n",
    "        metadata={\n",
    "            \"help\": \"URL of the code executor service\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    dump_dir: str = field(\n",
    "        default=\"./dump\",\n",
    "        metadata={\n",
    "            \"help\": \"directory to dump the trajectories\"\n",
    "        }\n",
    "    )\n",
    "    verbose: bool = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            \"help\": \"whether to print verbose information\"\n",
    "        }\n",
    "    )\n",
    "    recover_start_step: int = field(\n",
    "        default=0,\n",
    "        metadata={\n",
    "            \"help\": \"step to start recovering from, useful for resuming training\"\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abb861a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/liangchengwei/lcw/ZERO-TIR-RL/experiments/logs/liangchengwei/tir-grpo/trial0/generated'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config, _ = load_expr_config(args, AgentRLConfig)\n",
    "config: AgentRLConfig\n",
    "\n",
    "\n",
    "config.dump_dir = os.path.join(\n",
    "    StatsLogger.get_log_path(config.stats_logger), \"generated\"\n",
    ")\n",
    "\n",
    "config.dump_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fd04bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from areal.utils.network import find_free_ports\n",
    "\n",
    "SGLANG_PORT, MASTER_PORT = 11451, 14514\n",
    "\n",
    "SGLANG_HOST = \"127.0.0.1\"\n",
    "\n",
    "# Environment variables used by inference/train engines\n",
    "import os\n",
    "\n",
    "os.environ[\"AREAL_LLM_SERVER_ADDRS\"] = f\"{SGLANG_HOST}:{SGLANG_PORT}\"\n",
    "os.environ[\"MASTER_ADDR\"] = \"127.0.0.1\"\n",
    "os.environ[\"MASTER_PORT\"] = str(MASTER_PORT)\n",
    "os.environ[\"RANK\"] = str(0)\n",
    "os.environ[\"WORLD_SIZE\"] = str(1)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "os.environ[\"LOCAL_RANK\"] = str(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f31b06ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "# import sys\n",
    "# import threading\n",
    "# import time\n",
    "\n",
    "# # 启动sglang server\n",
    "# from areal.api.cli_args import SGLangConfig\n",
    "# from areal.utils.network import find_free_ports\n",
    "\n",
    "# config.sglang.log_level = \"info\"\n",
    "# config.sglang.decode_log_interval = 10\n",
    "# sglang_cmd = SGLangConfig.build_cmd(\n",
    "#     config.sglang,\n",
    "#     tp_size=1,\n",
    "#     base_gpu_id=1,\n",
    "#     host=SGLANG_HOST,\n",
    "#     port=SGLANG_PORT,\n",
    "# )\n",
    "\n",
    "# def read_pipe(pipe, prefix):\n",
    "#     \"\"\"实时读取管道输出并在notebook中显示\"\"\"\n",
    "#     for line in iter(pipe.readline, b''):\n",
    "#         try:\n",
    "#             line_str = line.decode('utf-8').rstrip()\n",
    "#             if line_str:\n",
    "#                 print(f\"[{prefix}] {line_str}\")\n",
    "#         except UnicodeDecodeError:\n",
    "#             print(f\"[{prefix}] <binary output>\")\n",
    "#     pipe.close()\n",
    "\n",
    "# sglang_process = subprocess.Popen(\n",
    "#     sglang_cmd,\n",
    "#     shell=True,\n",
    "#     stdout=subprocess.PIPE,\n",
    "#     stderr=subprocess.PIPE,\n",
    "#     bufsize=1,\n",
    "#     universal_newlines=False\n",
    "# )\n",
    "\n",
    "# # 获取并打印进程号\n",
    "# print(f\"SGLang服务器已启动，进程号(PID): {sglang_process.pid}\")\n",
    "\n",
    "# # 启动线程实时读取stdout和stderr\n",
    "# stdout_thread = threading.Thread(target=read_pipe, args=(sglang_process.stdout, \"STDOUT\"))\n",
    "# stderr_thread = threading.Thread(target=read_pipe, args=(sglang_process.stderr, \"STDERR\"))\n",
    "# stdout_thread.daemon = True\n",
    "# stderr_thread.daemon = True\n",
    "# stdout_thread.start()\n",
    "# stderr_thread.start()\n",
    "\n",
    "# print(\"SGLang服务器启动中，请等待初始化完成...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64290d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 56878\n",
      ">>> [{'from': 'human', 'value': 'Consider all 1000-element subsets of the set $\\\\{1, 2, 3, ... , 2015\\\\}$.  From each such subset choose the least element.  The arithmetic mean of all of these least elements is $\\\\frac{p}{q}$, where $p$ and $q$ are relatively prime positive integers.  Find $p + q$.'}, {'from': 'assistant', 'ground_truth': {'value': '431'}}]\n",
      ">>> {'question': 'Consider all 1000-element subsets of the set $\\\\{1, 2, 3, ... , 2015\\\\}$.  From each such subset choose the least element.  The arithmetic mean of all of these least elements is $\\\\frac{p}{q}$, where $p$ and $q$ are relatively prime positive integers.  Find $p + q$.', 'answer': '431'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('orz_math_57k_collected.json', 'r') as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "print(\">>>\", len(raw_data)) # 56878\n",
    "print(\">>>\", raw_data[2]) # [{'from': 'human', 'value': 'Consider all 1000-element subsets of the set $\\\\{1, 2, 3, ... , 2015\\\\}$.  From each such subset choose the least element.  The arithmetic mean of all of these least elements is $\\\\frac{p}{q}$, where $p$ and $q$ are relatively prime positive integers.  Find $p + q$.'}, {'from': 'assistant', 'ground_truth': {'value': '431'}}]\n",
    "\n",
    "def process_raw_data(item):\n",
    "    return {\n",
    "        \"question\": item[0]['value'],\n",
    "        \"answer\": item[1]['ground_truth']['value'],\n",
    "    }\n",
    "\n",
    "dataset = [process_raw_data(item) for item in raw_data]\n",
    "\n",
    "print(\">>>\", dataset[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11b3880f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 1\n",
      ">>> {'question': 'Find $a+b+c$ if the graph of the equation $y=ax^2+bx+c$ is a parabola with vertex $(5,3)$, vertical axis of symmetry, and contains the point $(2,0)$.', 'answer': '-\\\\frac73'}\n"
     ]
    }
   ],
   "source": [
    "dataloader = StatefulDataLoader(\n",
    "    dataset,\n",
    "    batch_size=config.train_dataset.batch_size,\n",
    "    shuffle=config.train_dataset.shuffle,\n",
    "    num_workers=config.train_dataset.num_workers,\n",
    "    collate_fn=lambda x: x,\n",
    "    drop_last=config.train_dataset.drop_last,\n",
    ")\n",
    "from itertools import cycle\n",
    "\n",
    "data_generator = cycle(dataloader)\n",
    "\n",
    "ft_spec = FinetuneSpec(\n",
    "    total_train_epochs=config.total_train_epochs,\n",
    "    dataset_size=len(dataloader) * config.train_dataset.batch_size,\n",
    "    train_batch_size=config.train_dataset.batch_size,\n",
    ")\n",
    "\n",
    "example_batch = next(data_generator)\n",
    "print(\">>>\", len(example_batch))\n",
    "print(\">>>\", example_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7fc705a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "rw_executor = ProcessPoolExecutor(max_workers=4)\n",
    "\n",
    "from realhf.impl.dataset.math_parser import extract_answer, math_equal\n",
    "\n",
    "REWARD_TIMEOUT_SECONDS = 15\n",
    "\n",
    "\n",
    "def reward_fn(generated, answer):\n",
    "    try:\n",
    "        x = extract_answer(generated, \"math\", use_last_number=True)\n",
    "        y = extract_answer(answer, \"math\", use_last_number=True)\n",
    "\n",
    "        if x is None or x.strip() in [\"None\", \"none\", \"\"]:\n",
    "            return 0.0\n",
    "        elif y is None or y.strip() in [\"None\", \"none\", \"\"]:\n",
    "            return 0.0\n",
    "        return float(math_equal(x, y, timeout=False))\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "# TODO: examine reward function\n",
    "reward_fn(\n",
    "    \"\\boxed{72}\",\n",
    "    \"Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55decc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import functools\n",
    "import os\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "import colorama\n",
    "import torch\n",
    "from tensordict import TensorDict\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizerFast\n",
    "\n",
    "from areal.api.cli_args import GenerationHyperparameters\n",
    "from areal.api.engine_api import InferenceEngine\n",
    "from areal.api.io_struct import (\n",
    "    AllocationMode,\n",
    "    FinetuneSpec,\n",
    "    ModelRequest,\n",
    "    WeightUpdateMeta,\n",
    ")\n",
    "from areal.api.workflow_api import RolloutWorkflow\n",
    "from areal.engine.ppo.actor import FSDPPPOActor\n",
    "from areal.engine.sglang_remote import RemoteSGLangEngine\n",
    "from areal.utils.data import concat_padded_tensors\n",
    "from areal.utils.device import log_gpu_stats\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_path)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "if tokenizer.pad_token_id not in config.gconfig.stop_token_ids:\n",
    "    config.gconfig.stop_token_ids.append(tokenizer.pad_token_id)\n",
    "if tokenizer.eos_token_id not in config.gconfig.stop_token_ids:\n",
    "    config.gconfig.stop_token_ids.append(tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32a403f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> ['72\\n<answer>82', 'line1\\nline2']\n",
      ">>> ['\\nimport numpy as np\\n\\nresult = 2**31 - 1\\nprint(result)\\n', '\\ndef foo(x):\\n    return x + 1\\nprint(foo(1))\\n', '\\nprint(123)\\nprint(foo(1))\\n', '\\nwith open(\"test.txt\", \"w\") as f:\\n    f.write(\"1 2 3\")\\n\\nwith open(\"test.txt\", \"r\") as f:\\n    content = f.read()\\n    print(content)\\n']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "answer_test_str = r\"\"\"\n",
    "<answer>72\n",
    "<answer>82</answer>\n",
    "<answer>line1\n",
    "line2</answer>\n",
    "\"\"\"\n",
    "\n",
    "answer_matches = re.findall(r'<answer>(.*?)</answer>', answer_test_str, re.DOTALL)\n",
    "\n",
    "print(\">>>\", answer_matches)\n",
    "\n",
    "code_test_str = r\"\"\"\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "result = 2**31 - 1\n",
    "print(result)\n",
    "```\n",
    "\n",
    "```python\n",
    "def foo(x):\n",
    "    return x + 1\n",
    "print(foo(1))\n",
    "```\n",
    "\n",
    "```python\n",
    "print(123)\n",
    "print(foo(1))\n",
    "```\n",
    "\n",
    "```python\n",
    "with open(\"test.txt\", \"w\") as f:\n",
    "    f.write(\"1 2 3\")\n",
    "\n",
    "with open(\"test.txt\", \"r\") as f:\n",
    "    content = f.read()\n",
    "    print(content)\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "code_matches = re.findall(r'```python(.*?)```', code_test_str, re.DOTALL)\n",
    "\n",
    "print(\">>>\", code_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a39860bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 2147483647\n",
      "\n",
      ">>> 2\n",
      "\n",
      ">>> 123\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/tmpl3oyv9xp/user_script.py\", line 6, in <module>\n",
      "    print(foo(1))\n",
      "          ^^^\n",
      "NameError: name 'foo' is not defined\n",
      "\n",
      ">>> 1 2 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from code_executor import execute_code\n",
    "for code in code_matches:\n",
    "    print(\">>>\", execute_code(code, timeout=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7450de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import aiohttp\n",
    "# from typing import Dict, Any\n",
    "# class CodeExecutorClient:\n",
    "#     def __init__(self, server_url: str, timeout = 10, max_retries: int = 3):\n",
    "#         self.server_url = server_url\n",
    "#         self.timeout = timeout\n",
    "#         self.max_retries = max_retries\n",
    "#         self.session = None\n",
    "    \n",
    "#     async def __aenter__(self):\n",
    "#         self.session = aiohttp.ClientSession()\n",
    "#         return self\n",
    "    \n",
    "#     async def __aexit__(self, exc_type, exc_val, exc_tb):\n",
    "#         if self.session:\n",
    "#             await self.session.close()\n",
    "    \n",
    "#     async def execute_code(self, code: str, timeout: int = 10, traj_rid=None) -> Dict[str, Any]:\n",
    "#         if not self.session:\n",
    "#             self.session = aiohttp.ClientSession()\n",
    "#         for _ in range(self.max_retries):\n",
    "#             try:\n",
    "#                 async with self.session.post(\n",
    "#                     f\"{self.server_url}/execute\",\n",
    "#                     json={\"code\": code, \"timeout\": timeout, \"traj_rid\": traj_rid},\n",
    "#                     timeout=aiohttp.ClientTimeout(total=timeout + 5)\n",
    "#                 ) as response:\n",
    "#                     result = await response.json()\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 result = {\n",
    "#                     \"success\": False,\n",
    "#                     \"stdout\": \"\",\n",
    "#                     \"stderr\": \"\",\n",
    "#                     \"error\": {\n",
    "#                         \"type\": type(e).__name__,\n",
    "#                         \"message\": str(e),\n",
    "#                         \"traceback\": \"\"\n",
    "#                     }\n",
    "#                 }\n",
    "#             if result.get(\"success\", False):\n",
    "#                 return result\n",
    "#         return result\n",
    "    \n",
    "#     async def health_check(self) -> bool:\n",
    "#         \"\"\"\n",
    "#         检查服务器是否健康\n",
    "        \n",
    "#         Returns:\n",
    "#             服务器是否可用\n",
    "#         \"\"\"\n",
    "#         if not self.session:\n",
    "#             self.session = aiohttp.ClientSession()\n",
    "        \n",
    "#         try:\n",
    "#             async with self.session.get(\n",
    "#                 f\"{self.server_url}/health\",\n",
    "#                 timeout=aiohttp.ClientTimeout(total=5)\n",
    "#             ) as response:\n",
    "#                 if response.status == 200:\n",
    "#                     result = await response.json()\n",
    "#                     return result.get(\"status\") == \"healthy\"\n",
    "#                 return False\n",
    "#         except:\n",
    "#             return False\n",
    "\n",
    "# code_executor = CodeExecutorClient(config.executor_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14680f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> True\n",
      ">>> {'success': True, 'content': '2147483647\\n'}\n",
      ">>> {'success': True, 'content': '2\\n'}\n",
      ">>> {'success': True, 'content': \"123\\nTraceback (most recent call last):\\n   line 3, in <module>\\nNameError: name 'foo' is not defined\\n\"}\n"
     ]
    }
   ],
   "source": [
    "# print(\">>>\", await code_executor.health_check())\n",
    "# for code in code_matches:\n",
    "#     print(\">>>\", await code_executor.execute_code(code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ec47d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> <|im_start|>system\n",
      "\n",
      "You are a helpful assistant. The User asks a question, and the Assistant solves it. \n",
      "The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. And your final answer will be extracted automatically by the \\boxed{{}} tag.\n",
      "In your reasoning-process, You can use python-code to solve your problem. Put the code within ```python and ``` tags. The script will be executed immediately and output will be returned.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "What is 1+1?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful assistant. The User asks a question, and the Assistant solves it. \n",
    "The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. And your final answer will be extracted automatically by the \\\\boxed{{}} tag.\n",
    "In your reasoning-process, You can use python-code to solve your problem. Put the code within ```python and ``` tags. The script will be executed immediately and output will be returned.\n",
    "\"\"\"\n",
    "\n",
    "def get_prompt(tokenizer, query):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    text += \"<think>\\n\"\n",
    "    return text\n",
    "\n",
    "example_prompt = get_prompt(tokenizer, \"What is 1+1?\")\n",
    "print(\">>>\", example_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dc28e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import threading\n",
    "from code_executor_client import execute_python_code\n",
    "\n",
    "class TIRWorkflow(RolloutWorkflow):\n",
    "    def __init__(\n",
    "        self, \n",
    "        config: AgentRLConfig, \n",
    "        tokenizer: PreTrainedTokenizerFast,\n",
    "        code_executor,\n",
    "    ):\n",
    "        self.config = config\n",
    "        self.gconfig = config.gconfig\n",
    "        self.tokenizer = tokenizer\n",
    "        self._qid_locks = {}\n",
    "        self._locks_lock = threading.Lock()\n",
    "        self.code_executor = code_executor\n",
    "        self.current_trajs = 0\n",
    "\n",
    "    def _get_qid_lock(self, qid):\n",
    "        with self._locks_lock:\n",
    "            if qid not in self._qid_locks:\n",
    "                self._qid_locks[qid] = threading.Lock()\n",
    "            return self._qid_locks[qid]\n",
    "\n",
    "    async def collect_agent_trajectory(self, qid, prompt, answer, engine):\n",
    "        traj_rid = uuid.uuid4().hex\n",
    "        loop = asyncio.get_event_loop()\n",
    "        result = None\n",
    "        reward = 0.0\n",
    "\n",
    "        num_turns = 0\n",
    "        input_str = prompt\n",
    "        input_ids = self.tokenizer.encode(input_str, add_special_tokens=False)\n",
    "        logprobs = [0.0] * len(input_ids)\n",
    "        loss_mask = [0] * len(input_ids)\n",
    "        stops = [\"```python\", \"</answer>\"]\n",
    "        total_gen_time = 0\n",
    "        total_exec_time = 0\n",
    "        start_time = time.time()\n",
    "        while num_turns < self.config.max_turns:\n",
    "            req = ModelRequest(\n",
    "                rid=traj_rid,\n",
    "                input_ids=input_ids,\n",
    "                gconfig=self.gconfig.new(n_samples=1),\n",
    "            )\n",
    "            req.gconfig.stop = stops\n",
    "            if len(input_ids) + self.gconfig.max_new_tokens >= self.config.max_tokens_per_traj:\n",
    "                break\n",
    "            \n",
    "            gen_start_time = time.time()\n",
    "            resp = await engine.agenerate(req)\n",
    "            gen_time = time.time() - gen_start_time\n",
    "            total_gen_time += gen_time\n",
    "            completion_str = self.tokenizer.decode(resp.output_tokens)\n",
    "\n",
    "            input_str += completion_str\n",
    "            input_ids += resp.output_tokens\n",
    "            logprobs += resp.output_logprobs\n",
    "            loss_mask += [1] * len(resp.output_tokens)\n",
    "\n",
    "            if \"</answer>\" in completion_str:\n",
    "                matches = re.findall(r\"<answer>(.*?)</answer>\", completion_str, re.DOTALL)\n",
    "                if matches:\n",
    "                    result = matches[-1]\n",
    "                    reward = await loop.run_in_executor(\n",
    "                        rw_executor,\n",
    "                        functools.partial(reward_fn, result, answer)\n",
    "                    )\n",
    "                    break\n",
    "            elif stops[0] == \"```python\" and \"```python\" in completion_str:\n",
    "                stops[0] = \"```\"\n",
    "            elif stops[0] == \"```\" and \"```\" in completion_str:\n",
    "                matches = re.findall(r'```python(.*?)```', input_str, re.DOTALL)\n",
    "                if matches:\n",
    "                    code = matches[-1]\n",
    "                    exec_start_time = time.time()\n",
    "                    exec_result = await self.code_executor.execute_code(code, traj_rid=traj_rid)\n",
    "                    exec_time = time.time() - exec_start_time\n",
    "                    total_exec_time += exec_time\n",
    "                    \n",
    "                    if exec_result[\"success\"]:\n",
    "                        execution_output = exec_result[\"content\"]\n",
    "                    else:\n",
    "                        # 服务端错误\n",
    "                        logger.error(f\"Code execution failed: {exec_result['error']['message']}\")\n",
    "                        execution_output = \"代码执行失败。\"\n",
    "                    \n",
    "                    num_turns += 1\n",
    "                    execution_output = \"\\n```output\\n\" + execution_output + \"\\n```\\n\"\n",
    "                    input_str += execution_output\n",
    "                    exec_tokens = self.tokenizer.encode(execution_output, add_special_tokens=False)\n",
    "                    if len(input_ids) + len(exec_tokens) >= self.config.max_tokens_per_traj:\n",
    "                        exec_tokens = exec_tokens[:self.config.max_tokens_per_traj - len(input_ids) - 1]\n",
    "                    input_ids += exec_tokens\n",
    "                    logprobs += [0.0] * len(exec_tokens)\n",
    "                    loss_mask += [0] * len(exec_tokens)\n",
    "                stops[0] = \"```python\"\n",
    "            \n",
    "            if resp.output_tokens[-1] in [self.tokenizer.eos_token_id, self.tokenizer.pad_token_id]:\n",
    "                break\n",
    "\n",
    "        total_time = time.time() - start_time\n",
    "\n",
    "        if len(input_ids) > self.config.max_tokens_per_traj:\n",
    "            assert False, f\"Trajectory {traj_rid} exceeds max tokens {self.config.max_tokens_per_traj} with {len(input_ids)} tokens.\"\n",
    "        \n",
    "        res = dict(\n",
    "            input_ids=torch.tensor(input_ids),\n",
    "            logprobs=torch.tensor(logprobs),\n",
    "            loss_mask=torch.tensor(loss_mask, dtype=torch.bool),\n",
    "            rewards=torch.tensor(float(reward)),\n",
    "            code_reward=torch.tensor(float(num_turns>0)),\n",
    "            code_in_correct=torch.tensor(float(num_turns>0 and reward>0)),\n",
    "            attention_mask=torch.ones(len(input_ids), dtype=torch.bool),\n",
    "        )\n",
    "\n",
    "        res_dump = {k: v.tolist() for k, v in res.items() if k != 'attention_mask' and k != 'rewards'}\n",
    "        res_dump['input_str'] = input_str\n",
    "        res_dump['metadata'] = {\n",
    "            \"reward\": reward,\n",
    "            \"traj_rid\": traj_rid,\n",
    "            \"num_turns\": num_turns,\n",
    "            \"length\": len(input_ids),\n",
    "            \"total_time\": f\"{total_time:.2f}s\",\n",
    "            \"gen_time_ratio\": f\"{total_gen_time / total_time:.2f}\" if total_time > 0 else \"0.00\",\n",
    "            \"exec_time_ratio\": f\"{total_exec_time / total_time:.2f}\" if total_time > 0 else \"0.00\",\n",
    "            \"answer\": answer,\n",
    "            \"result\": result,\n",
    "        }\n",
    "\n",
    "        res = {k: v.unsqueeze(0) for k, v in res.items()}\n",
    "        return TensorDict(res, batch_size=[1])\n",
    "\n",
    "    async def arun_episode(self, engine, data):\n",
    "        qid = uuid.uuid4().hex\n",
    "\n",
    "        # prompt = PROMPT_TEMPLATE.format(query=data[\"question\"])\n",
    "        prompt = get_prompt(self.tokenizer, data[\"question\"])\n",
    "\n",
    "        trajs = await asyncio.gather(*[\n",
    "            self.collect_agent_trajectory(qid, prompt, data[\"answer\"], engine)\n",
    "            for _ in range(self.config.n_trajs)\n",
    "        ])\n",
    "\n",
    "\n",
    "        \n",
    "        return concat_padded_tensors(trajs)\n",
    "\n",
    "workflow = TIRWorkflow(\n",
    "    config=config,\n",
    "    tokenizer=tokenizer,\n",
    "    code_executor=code_executor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66c360a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m20250830-17:58:13.117 areal.engine.sglang_remote INFO: Waiting for server ready...\u001b[0m\n",
      "\u001b[37m20250830-17:58:13.124 areal.engine.sglang_remote INFO: Servers are all ready!\u001b[0m\n",
      "\u001b[31m20250830-17:58:18.685 TIR ERROR: Code execution failed: Task <Task pending name='Task-55' coro=<TCPConnector._resolve_host_with_throttle() running at /home/liangchengwei/miniconda3/envs/test/lib/python3.12/site-packages/aiohttp/connector.py:1179>> got Future <Future pending cb=[_chain_future.<locals>._call_check_cancel() at /home/liangchengwei/miniconda3/envs/test/lib/python3.12/asyncio/futures.py:389]> attached to a different loop\u001b[0m\n",
      ">>> TensorDict(\n",
      "    fields={\n",
      "        attention_mask: Tensor(shape=torch.Size([16, 4000]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        code_in_correct: Tensor(shape=torch.Size([16]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        code_reward: Tensor(shape=torch.Size([16]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        input_ids: Tensor(shape=torch.Size([16, 4000]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        logprobs: Tensor(shape=torch.Size([16, 4000]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        loss_mask: Tensor(shape=torch.Size([16, 4000]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        rewards: Tensor(shape=torch.Size([16]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "    batch_size=torch.Size([16]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "rollout = RemoteSGLangEngine(config.rollout)\n",
    "rollout.initialize(None, None)\n",
    "try:\n",
    "    sample_data = next(data_generator)[:2]\n",
    "    res = rollout.rollout_batch(sample_data, workflow=workflow)\n",
    "    print(\">>>\", res)\n",
    "finally:\n",
    "    # rollout.destroy()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd11a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 151644 token: <|im_start|>                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 8948 token: system                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 271 token: \n",
      "\n",
      "                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 2610 token: You                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 525 token:  are                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 264 token:  a                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 10950 token:  helpful                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 17847 token:  assistant                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 13 token: .                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 576 token:  The                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 2657 token:  User                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 17064 token:  asks                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 264 token:  a                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 3405 token:  question                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 11 token: ,                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 323 token:  and                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 279 token:  the                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 21388 token:  Assistant                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 67477 token:  solves                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 432 token:  it                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 13 token: .                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 715 token:  \n",
      "                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 785 token: The                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 21388 token:  Assistant                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1156 token:  first                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 15482 token:  thinks                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 911 token:  about                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 279 token:  the                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 32711 token:  reasoning                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1882 token:  process                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 304 token:  in                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 279 token:  the                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 3971 token:  mind                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 323 token:  and                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1221 token:  then                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 5707 token:  provides                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 279 token:  the                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 2657 token:  User                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 448 token:  with                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 279 token:  the                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 4226 token:  answer                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 13 token: .                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 576 token:  The                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 32711 token:  reasoning                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1882 token:  process                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 374 token:  is                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 43810 token:  enclosed                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 2878 token:  within                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 366 token:  <                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 26865 token: think                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 29 token: >                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 690 token:  </                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 26865 token: think                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 29 token: >                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 323 token:  and                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 4226 token:  answer                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 374 token:  is                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 43810 token:  enclosed                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 2878 token:  within                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 366 token:  <                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 9217 token: answer                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 29 token: >                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 690 token:  </                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 9217 token: answer                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 29 token: >                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 9492 token:  tags                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 11 token: ,                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 15576 token:  respectively                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 11 token: ,                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 600 token:  i                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1734 token: .e                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 2572 token: .,                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 366 token:  <                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 26865 token: think                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 29 token: >                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 32711 token:  reasoning                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1882 token:  process                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1588 token:  here                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 690 token:  </                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 26865 token: think                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 29 token: >                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 366 token:  <                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 9217 token: answer                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 29 token: >                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 4226 token:  answer                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1588 token:  here                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 690 token:  </                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 9217 token: answer                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 14276 token: >.                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1597 token:  And                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 697 token:  your                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1590 token:  final                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 4226 token:  answer                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 686 token:  will                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 387 token:  be                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 27432 token:  extracted                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 9463 token:  automatically                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 553 token:  by                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 279 token:  the                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1124 token:  \\                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 79075 token: boxed                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 2979 token: {{                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 3417 token: }}                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 4772 token:  tag                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 624 token: .\n",
      "                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 641 token: In                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 697 token:  your                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 32711 token:  reasoning                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 50094 token: -process                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 11 token: ,                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1446 token:  You                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 646 token:  can                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 990 token:  use                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 10135 token:  python                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 25261 token: -code                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 311 token:  to                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 11625 token:  solve                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 697 token:  your                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 3491 token:  problem                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 11 token: ,                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 323 token:  and                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 279 token:  the                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 2038 token:  code                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 686 token:  will                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 387 token:  be                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 15695 token:  executed                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 7069 token:  immediately                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 323 token:  and                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1172 token:  only                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 279 token:  the                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 20075 token:  stdout                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 323 token:  and                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 26436 token:  stderr                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 686 token:  will                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 387 token:  be                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 5927 token:  returned                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 11 token: ,                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 773 token:  so                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 498 token:  you                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1265 token:  should                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 990 token:  use                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1565 token:  `                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1350 token: print                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 63 token: `                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 304 token:  in                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 279 token:  the                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 2038 token:  code                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 624 token: .\n",
      "                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 151645 token: <|im_end|>                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 198 token: \n",
      "                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 151644 token: <|im_start|>                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 872 token: user                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 198 token: \n",
      "                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 3838 token: What                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 374 token:  is                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 279 token:  the                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 7772 token:  largest                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 10250 token:  prime                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 8168 token:  factor                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 315 token:  of                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 400 token:  $                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 20 token: 5                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 488 token:  +                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 220 token:                    logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 21 token: 6                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 3 token: $                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 30 token: ?                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 151645 token: <|im_end|>                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 198 token: \n",
      "                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 151644 token: <|im_start|>                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 77091 token: assistant                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 198 token: \n",
      "                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 13708 token: <th                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 766 token: ink                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 397 token: >\n",
      "                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1249 token: To                  logprob: -0.1408 loss_mask: True attention_mask: True\n",
      "id: 1477 token:  find                  logprob: -0.1264 loss_mask: True attention_mask: True\n",
      "id: 279 token:  the                  logprob: -0.0037 loss_mask: True attention_mask: True\n",
      "id: 4226 token:  answer                  logprob: -7.4083 loss_mask: True attention_mask: True\n",
      "id: 11 token: ,                  logprob: -0.0256 loss_mask: True attention_mask: True\n",
      "id: 1077 token:  let                  logprob: -2.2723 loss_mask: True attention_mask: True\n",
      "id: 594 token: 's                  logprob: -0.0167 loss_mask: True attention_mask: True\n",
      "id: 1156 token:  first                  logprob: -0.4997 loss_mask: True attention_mask: True\n",
      "id: 39721 token:  simplify                  logprob: -1.8853 loss_mask: True attention_mask: True\n",
      "id: 279 token:  the                  logprob: -0.1981 loss_mask: True attention_mask: True\n",
      "id: 7493 token:  expression                  logprob: -0.1123 loss_mask: True attention_mask: True\n",
      "id: 323 token:  and                  logprob: -3.0524 loss_mask: True attention_mask: True\n",
      "id: 11047 token:  calculate                  logprob: -2.8531 loss_mask: True attention_mask: True\n",
      "id: 279 token:  the                  logprob: -0.5850 loss_mask: True attention_mask: True\n",
      "id: 10250 token:  prime                  logprob: -2.3584 loss_mask: True attention_mask: True\n",
      "id: 9363 token:  factors                  logprob: -0.0796 loss_mask: True attention_mask: True\n",
      "id: 315 token:  of                  logprob: -0.2765 loss_mask: True attention_mask: True\n",
      "id: 279 token:  the                  logprob: -0.6847 loss_mask: True attention_mask: True\n",
      "id: 1102 token:  result                  logprob: -0.7495 loss_mask: True attention_mask: True\n",
      "id: 624 token: .\n",
      "                  logprob: -0.2707 loss_mask: True attention_mask: True\n",
      "id: 522 token: </                  logprob: -0.0216 loss_mask: True attention_mask: True\n",
      "id: 26865 token: think                  logprob: -0.0004 loss_mask: True attention_mask: True\n",
      "id: 397 token: >\n",
      "                  logprob: -0.5184 loss_mask: True attention_mask: True\n",
      "id: 27 token: <                  logprob: -0.0164 loss_mask: True attention_mask: True\n",
      "id: 9217 token: answer                  logprob: -0.0018 loss_mask: True attention_mask: True\n",
      "id: 397 token: >\n",
      "                  logprob: -0.1016 loss_mask: True attention_mask: True\n",
      "id: 21 token: 6                  logprob: -6.3549 loss_mask: True attention_mask: True\n",
      "id: 0 token: !                  logprob: -0.7476 loss_mask: True attention_mask: True\n",
      "id: 284 token:  =                  logprob: -0.4665 loss_mask: True attention_mask: True\n",
      "id: 220 token:                    logprob: -0.0162 loss_mask: True attention_mask: True\n",
      "id: 22 token: 7                  logprob: -0.0655 loss_mask: True attention_mask: True\n",
      "id: 17 token: 2                  logprob: -0.0001 loss_mask: True attention_mask: True\n",
      "id: 15 token: 0                  logprob: -0.0001 loss_mask: True attention_mask: True\n",
      "id: 11 token: ,                  logprob: -0.6759 loss_mask: True attention_mask: True\n",
      "id: 8450 token:  thus                  logprob: -4.3052 loss_mask: True attention_mask: True\n",
      "id: 400 token:  $                  logprob: -1.1849 loss_mask: True attention_mask: True\n",
      "id: 20 token: 5                  logprob: -0.0055 loss_mask: True attention_mask: True\n",
      "id: 0 token: !                  logprob: -0.0035 loss_mask: True attention_mask: True\n",
      "id: 488 token:  +                  logprob: -0.0152 loss_mask: True attention_mask: True\n",
      "id: 220 token:                    logprob: -0.0002 loss_mask: True attention_mask: True\n",
      "id: 21 token: 6                  logprob: -0.0053 loss_mask: True attention_mask: True\n",
      "id: 0 token: !                  logprob: -0.0011 loss_mask: True attention_mask: True\n",
      "id: 284 token:  =                  logprob: -0.0248 loss_mask: True attention_mask: True\n",
      "id: 220 token:                    logprob: -0.0044 loss_mask: True attention_mask: True\n",
      "id: 20 token: 5                  logprob: -0.4261 loss_mask: True attention_mask: True\n",
      "id: 0 token: !                  logprob: -0.1541 loss_mask: True attention_mask: True\n",
      "id: 488 token:  +                  logprob: -0.0523 loss_mask: True attention_mask: True\n",
      "id: 220 token:                    logprob: -0.0066 loss_mask: True attention_mask: True\n",
      "id: 22 token: 7                  logprob: -0.0892 loss_mask: True attention_mask: True\n",
      "id: 17 token: 2                  logprob: -0.0003 loss_mask: True attention_mask: True\n",
      "id: 15 token: 0                  logprob: -0.0003 loss_mask: True attention_mask: True\n",
      "id: 284 token:  =                  logprob: -0.4337 loss_mask: True attention_mask: True\n",
      "id: 220 token:                    logprob: -0.0278 loss_mask: True attention_mask: True\n",
      "id: 16 token: 1                  logprob: -0.1373 loss_mask: True attention_mask: True\n",
      "id: 17 token: 2                  logprob: -0.0061 loss_mask: True attention_mask: True\n",
      "id: 15 token: 0                  logprob: -0.0003 loss_mask: True attention_mask: True\n",
      "id: 488 token:  +                  logprob: -0.0397 loss_mask: True attention_mask: True\n",
      "id: 220 token:                    logprob: -0.0007 loss_mask: True attention_mask: True\n",
      "id: 22 token: 7                  logprob: -0.0008 loss_mask: True attention_mask: True\n",
      "id: 17 token: 2                  logprob: -0.0001 loss_mask: True attention_mask: True\n",
      "id: 15 token: 0                  logprob: -0.0001 loss_mask: True attention_mask: True\n",
      "id: 284 token:  =                  logprob: -0.0072 loss_mask: True attention_mask: True\n",
      "id: 220 token:                    logprob: -0.0003 loss_mask: True attention_mask: True\n",
      "id: 23 token: 8                  logprob: -0.0050 loss_mask: True attention_mask: True\n",
      "id: 19 token: 4                  logprob: -0.0001 loss_mask: True attention_mask: True\n",
      "id: 15 token: 0                  logprob: -0.0000 loss_mask: True attention_mask: True\n",
      "id: 12947 token: $.                  logprob: -0.7294 loss_mask: True attention_mask: True\n",
      "id: 576 token:  The                  logprob: -1.3577 loss_mask: True attention_mask: True\n",
      "id: 10250 token:  prime                  logprob: -0.2020 loss_mask: True attention_mask: True\n",
      "id: 9363 token:  factors                  logprob: -0.3199 loss_mask: True attention_mask: True\n",
      "id: 315 token:  of                  logprob: -0.0214 loss_mask: True attention_mask: True\n",
      "id: 220 token:                    logprob: -0.0368 loss_mask: True attention_mask: True\n",
      "id: 23 token: 8                  logprob: -0.0022 loss_mask: True attention_mask: True\n",
      "id: 19 token: 4                  logprob: -0.0001 loss_mask: True attention_mask: True\n",
      "id: 15 token: 0                  logprob: -0.0000 loss_mask: True attention_mask: True\n",
      "id: 525 token:  are                  logprob: -0.0305 loss_mask: True attention_mask: True\n",
      "id: 220 token:                    logprob: -0.0658 loss_mask: True attention_mask: True\n",
      "id: 17 token: 2                  logprob: -0.0126 loss_mask: True attention_mask: True\n",
      "id: 11 token: ,                  logprob: -0.0794 loss_mask: True attention_mask: True\n",
      "id: 220 token:                    logprob: -0.0025 loss_mask: True attention_mask: True\n",
      "id: 18 token: 3                  logprob: -0.0077 loss_mask: True attention_mask: True\n",
      "id: 11 token: ,                  logprob: -0.0041 loss_mask: True attention_mask: True\n",
      "id: 323 token:  and                  logprob: -1.3134 loss_mask: True attention_mask: True\n",
      "id: 220 token:                    logprob: -0.0020 loss_mask: True attention_mask: True\n",
      "id: 20 token: 5                  logprob: -0.1459 loss_mask: True attention_mask: True\n",
      "id: 13 token: .                  logprob: -0.6958 loss_mask: True attention_mask: True\n",
      "id: 576 token:  The                  logprob: -0.6248 loss_mask: True attention_mask: True\n",
      "id: 7772 token:  largest                  logprob: -0.0151 loss_mask: True attention_mask: True\n",
      "id: 10250 token:  prime                  logprob: -0.3581 loss_mask: True attention_mask: True\n",
      "id: 8168 token:  factor                  logprob: -0.0032 loss_mask: True attention_mask: True\n",
      "id: 374 token:  is                  logprob: -0.2835 loss_mask: True attention_mask: True\n",
      "id: 220 token:                    logprob: -0.0909 loss_mask: True attention_mask: True\n",
      "id: 20 token: 5                  logprob: -0.0085 loss_mask: True attention_mask: True\n",
      "id: 624 token: .\n",
      "                  logprob: -0.0628 loss_mask: True attention_mask: True\n",
      "id: 522 token: </                  logprob: -0.0015 loss_mask: True attention_mask: True\n",
      "id: 9217 token: answer                  logprob: -0.0001 loss_mask: True attention_mask: True\n",
      "id: 29 token: >                  logprob: -0.0021 loss_mask: True attention_mask: True\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(res[0]['input_ids'])):\n",
    "    print(f\"id: {res[0]['input_ids'][i]} token: {tokenizer.decode(res[0]['input_ids'][i])}                  logprob: {res[0]['logprobs'][i]:.4f} loss_mask: {res[0]['loss_mask'][i]} attention_mask: {res[0]['attention_mask'][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8207420a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        attention_mask: Tensor(shape=torch.Size([16, 706]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        code_in_correct: Tensor(shape=torch.Size([16]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        code_reward: Tensor(shape=torch.Size([16]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        input_ids: Tensor(shape=torch.Size([16, 706]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        logprobs: Tensor(shape=torch.Size([16, 706]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        loss_mask: Tensor(shape=torch.Size([16, 706]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        rewards: Tensor(shape=torch.Size([16]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        score: Tensor(shape=torch.Size([16]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "    batch_size=torch.Size([16]),\n",
       "    device=None,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"debug_batch.pkl\", \"rb\") as f:\n",
    "    import pickle\n",
    "    debug_batch = pickle.load(f)\n",
    "\n",
    "debug_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "906bd0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> <|im_start|>system\n",
      "\n",
      "You are a helpful assistant. The User asks a question, and the Assistant solves it. \n",
      "The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. And your final answer will be extracted automatically by the \\boxed{{}} tag.\n",
      "In your reasoning-process, You can use python-code to solve your problem. Put the code within ```python and ``` tags. The script will be executed immediately and output will be returned.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "Suppose that we have a right triangle $DEF$ with the right angle at $E$ such that $DF = \\sqrt{85}$ and $DE = 7$. A circle is drawn with its center on $DE$ such that the circle is tangent to $DF$ and $EF$. If $Q$ is the point where the circle and side $DF$ meet, then what is $FQ$?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "Given numbers, the first step is to find the length of the hypotenuse, which is known as $EF$. Using Pythagoras' theorem to calculate $EF$ is lengthy but straightforward. After knowing $EF$ and recognizing that triangle $DEF$ is right-angled at $D$, we can calculate the tangent, $FQ$, by considering the geometrical properties of similar triangles.\n",
      "\n",
      "```python\n",
      "# Given numbers\n",
      "DF = sqrt(85)\n",
      "DE = 7\n",
      "\n",
      "# Calculate EF using Pythagoras' theorem\n",
      "EF = sqrt(DF**2 + DE**2)\n",
      "\n",
      "# Calculate FQ = sqrt(85)/tan(pi()/2)\n",
      "xFQ = sqrt(85) / tan(pi()/2)\n",
      "xFQ\n",
      "```\n",
      "\n",
      "```output\n",
      "Traceback (most recent call last):\n",
      "   line 3, in <module>\n",
      "NameError: name 'sqrt' is not defined\n",
      "\n",
      "```\n",
      "</answer>\n",
      "```<|im_end|>!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      ">>> tensor(-0.1250)\n",
      ">>> tensor(0.)\n",
      ">>> <|im_start|>system\n",
      "\n",
      "You are a helpful assistant. The User asks a question, and the Assistant solves it. \n",
      "The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. And your final answer will be extracted automatically by the \\boxed{{}} tag.\n",
      "In your reasoning-process, You can use python-code to solve your problem. Put the code within ```python and ``` tags. The script will be executed immediately and output will be returned.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "Suppose that we have a right triangle $DEF$ with the right angle at $E$ such that $DF = \\sqrt{85}$ and $DE = 7$. A circle is drawn with its center on $DE$ such that the circle is tangent to $DF$ and $EF$. If $Q$ is the point where the circle and side $DF$ meet, then what is $FQ$?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "We can use the Pythagorean Theorem to find the length of $EF$. Then, we can use similar triangles to find the length of $FQ$.\n",
      "</think>\n",
      "\n",
      "<answer>\n",
      "```python\n",
      "from sympy import symbols, sqrt, Eq, solve\n",
      "\n",
      "DF, DE = sqrt(85), 7  # sides of the right triangle DEF in a Pythagorean triple of sqrt(85), 7, 8\n",
      "EF = symbols('EF')\n",
      "\n",
      "# Using the Pythagorean Theorem\n",
      "eq1 = Eq(DF**2 + DE**2, EF**2)\n",
      "\n",
      "# Solve for EF\n",
      "EF_solution = solve(eq1, EF)\n",
      "\n",
      "EF_solution[0]\n",
      "```\n",
      "\n",
      "\n",
      "```output\n",
      "\n",
      "```\n",
      "No output. Let me use `print()` to see the result and try again.\n",
      "```python\n",
      "```\n",
      "```output\n",
      "\n",
      "```\n",
      "No output. Let me use `print()` to see the result and try again.\n",
      "```python\n",
      "```\n",
      "```output\n",
      "\n",
      "```\n",
      "No output. Let me use `print()` to see the result and try again.\n",
      "```python\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "```output\n",
      "\n",
      "```\n",
      "No output. Let me use `print()` to see the result and try again.\n",
      "```\n",
      "\n",
      "```execution_result\n",
      "\n",
      "```<|im_end|>!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      ">>> tensor(-0.1250)\n",
      ">>> tensor(0.)\n",
      ">>> <|im_start|>system\n",
      "\n",
      "You are a helpful assistant. The User asks a question, and the Assistant solves it. \n",
      "The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. And your final answer will be extracted automatically by the \\boxed{{}} tag.\n",
      "In your reasoning-process, You can use python-code to solve your problem. Put the code within ```python and ``` tags. The script will be executed immediately and output will be returned.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "Suppose that we have a right triangle $DEF$ with the right angle at $E$ such that $DF = \\sqrt{85}$ and $DE = 7$. A circle is drawn with its center on $DE$ such that the circle is tangent to $DF$ and $EF$. If $Q$ is the point where the circle and side $DF$ meet, then what is $FQ$?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "To solve this problem, we first need to determine the radius of the circle. The circle is tangent to $DF$ at a point on it, so its radius is the distance from the center of the circle to this point, which is $FQ$.\n",
      "\n",
      "Next, we can use the Law of Cosines to find an expression for $EF$ based on $DF$, $DE$, and the unknown angle at $F$. Knowing that $FQ$ and $DE$ are equal in length because they are both parts of a common side, we can use these to set up an equation in terms of $FQ$.\n",
      "\n",
      "Finally, by recognizing that the radius of the circle is also equal to $FQ$ (since it's tangent to $DF$), we can find $FQ$ directly from the Law of Cosines expression.\n",
      "</think>\n",
      "\n",
      "<answer>\n",
      "```python\n",
      "# Given values\n",
      "DF = 7\n",
      "radius = DF\n",
      "\n",
      "# Using Pythagorean theorem to find EF\n",
      "EF = sqrt(DF**2 - DE**2)\n",
      "\n",
      "# Using the right triangle FQD\n",
      "FQ = radius\n",
      "\n",
      "# Check if EF is the circumradius of triangle FQD\n",
      "circumradius = (DF**2 + DE**2 + EF**2)**0.5 / 2\n",
      "\n",
      "# Check if the circumradius matches our result\n",
      "if circumradius == FQ:\n",
      "else:\n",
      "    print(\"The circumradius calculation does not match the found FQ, indicating a possible error in the calculations.\")\n",
      "```\n",
      "\n",
      "```output\n",
      "Traceback (most recent call last):\n",
      "   line 17\n",
      "    else:\n",
      "    ^^^^\n",
      "IndentationError: expected an indented block after 'if' statement on line 16\n",
      "\n",
      "```\n",
      "Therefore, the answer is floating, that's code and environment configuration to make it error. But, we can definitely do this using some manual geometry involving Pythagoras theorem and trigonometric ratio. The correct answer is `FQ = radius = 7`.\n",
      "</answer><|im_end|>!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      ">>> tensor(-0.1250)\n",
      ">>> tensor(0.)\n",
      ">>> <|im_start|>system\n",
      "\n",
      "You are a helpful assistant. The User asks a question, and the Assistant solves it. \n",
      "The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. And your final answer will be extracted automatically by the \\boxed{{}} tag.\n",
      "In your reasoning-process, You can use python-code to solve your problem. Put the code within ```python and ``` tags. The script will be executed immediately and output will be returned.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "Suppose that we have a right triangle $DEF$ with the right angle at $E$ such that $DF = \\sqrt{85}$ and $DE = 7$. A circle is drawn with its center on $DE$ such that the circle is tangent to $DF$ and $EF$. If $Q$ is the point where the circle and side $DF$ meet, then what is $FQ$?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "Given the right triangle with its hypotenuse DE and the angle at E, we can form a system of equations based on the Pythagorean theorem. The general solution of these equations enables us to calculate the length of side FQ. \n",
      "</think>\n",
      "\n",
      "<answer>\n",
      "By applying Pythagoras to triangle DEF (3-4-5), we find the other leg DF measures (7 - sqrt(85)). The circle with radius QR is tangent to DF and EF, connecting our points. Since the circle is circumscribed around triangle DEQ, with Q being the intersection point, we apply the Pythagorean theorem to triangle EQQ' (with Q' being the reference point due to the tangent circle duplication) to find FQ.\n",
      "QED.<|im_end|>!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      ">>> tensor(-0.1250)\n",
      ">>> tensor(0.)\n",
      ">>> <|im_start|>system\n",
      "\n",
      "You are a helpful assistant. The User asks a question, and the Assistant solves it. \n",
      "The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. And your final answer will be extracted automatically by the \\boxed{{}} tag.\n",
      "In your reasoning-process, You can use python-code to solve your problem. Put the code within ```python and ``` tags. The script will be executed immediately and output will be returned.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "Suppose that we have a right triangle $DEF$ with the right angle at $E$ such that $DF = \\sqrt{85}$ and $DE = 7$. A circle is drawn with its center on $DE$ such that the circle is tangent to $DF$ and $EF$. If $Q$ is the point where the circle and side $DF$ meet, then what is $FQ$?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "To solve this problem, we need to use the fact that a circle inscribed in a right triangle is tangent to the sides of the triangle at their right angle, which creates two 45-45-90 triangles, where the hypotenuse is opposite the right angle.\n",
      "</think>\n",
      "<answer>\n",
      "Since the circle is inscribed in a right triangle, it is tangent to the two sides of the triangle forming the right angle. In this case, the two sides are $DF = \\sqrt{85}$ and $DE = 7$. The circle is tangent to $DF$ at $P$, and it is tangent to $EF$ at $Q$. \n",
      "\n",
      "Now, let's analyze the two right triangles created by the circle and side $DF$. The hypotenuse of each of these triangles is $DF$, and the adjacent side is $DE$. We can use the Pythagorean theorem to express $FQ$ as a function of the two sides $DE$ and $DF$. \n",
      "\n",
      "In terms of $DE$:\n",
      "\\[FQ = \\sqrt{DE^2 - DF^2} = \\sqrt{7^2 - \\sqrt{85}^2} = \\sqrt{49 - 85} = \\sqrt{-36} = -6.\\]\n",
      "\n",
      "However, considering the length units, we should take the absolute value. The absolute value of a number is its distance from zero on the number line, so $-6$ corresponds to $6$ since we're taking the square root of a regular number and not an imaginary number.\n",
      "\n",
      "Therefore, the length of $FQ$ is $6$.\n",
      "</answer>!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      ">>> tensor(0.8750)\n",
      ">>> tensor(1.)\n",
      ">>> <|im_start|>system\n",
      "\n",
      "You are a helpful assistant. The User asks a question, and the Assistant solves it. \n",
      "The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. And your final answer will be extracted automatically by the \\boxed{{}} tag.\n",
      "In your reasoning-process, You can use python-code to solve your problem. Put the code within ```python and ``` tags. The script will be executed immediately and output will be returned.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "Suppose that we have a right triangle $DEF$ with the right angle at $E$ such that $DF = \\sqrt{85}$ and $DE = 7$. A circle is drawn with its center on $DE$ such that the circle is tangent to $DF$ and $EF$. If $Q$ is the point where the circle and side $DF$ meet, then what is $FQ$?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "Given that the right triangle $DEF$ with its hypotenuse $EF$ is given and $DF = \\sqrt{85}$, and $DE = 7$, the intersection of the circle and side $DF$ is at point $Q$. The center of the circle is on $DE$, which means the distance between $D$ and $E$ is the same as the diameter of the circle. Therefore, we can find $EF$ through the Pythagorean theorem.\n",
      "</think>\n",
      "\n",
      "<answer>\n",
      "The diameter of the circle is $EF = \\sqrt{DF^2 + DE^2} = \\sqrt{85^2 + 7^2} = \\sqrt{7225 + 49} = \\sqrt{7274} = 86$. Since $Q$ is the intersection of the circle and side $DF$, $FQ$ is the difference between the diameter of the circle and DF. Thus, $FQ = EF - DF = 86 - \\sqrt{85}$.\n",
      "</answer>!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      ">>> tensor(-0.1250)\n",
      ">>> tensor(0.)\n",
      ">>> <|im_start|>system\n",
      "\n",
      "You are a helpful assistant. The User asks a question, and the Assistant solves it. \n",
      "The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. And your final answer will be extracted automatically by the \\boxed{{}} tag.\n",
      "In your reasoning-process, You can use python-code to solve your problem. Put the code within ```python and ``` tags. The script will be executed immediately and output will be returned.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "Suppose that we have a right triangle $DEF$ with the right angle at $E$ such that $DF = \\sqrt{85}$ and $DE = 7$. A circle is drawn with its center on $DE$ such that the circle is tangent to $DF$ and $EF$. If $Q$ is the point where the circle and side $DF$ meet, then what is $FQ$?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "To find the length of $FQ$, one must first understand that by the Pythagorean Theorem, the length of the hypotenuse $EF$ can be found. Furthermore, the radius of the circle is a central angle between $Q$ and the hypotenuse $EF$, which can be determined if one knows the length of the tangent segments $DF$ and $EF$.\n",
      "</think>\n",
      "<answer>By the Pythagorean Theorem, one can find that $EF = \\sqrt{DE^2 + DF^2}$. Plugging $DE=7$ and $DF = \\sqrt{85}$ into the equation above gives $EF = \\sqrt{49 + 63.5} = \\sqrt{112.5}$. Since $Q$ and the circle's center lie on the hypotenuse $EF$, the circle's radius, let's call it $r$, is $\\frac{EF}{2}$. By the definition of tangent, $r$ is also equal to the distance from the center $O$ of the circle to $Q$, which we'll call $OP$. Using the Pythagorean Theorem again, $OP^2 = OQ^2 - r^2$. Since $OF = r$, we can express $OQ$ and $OP$ in terms of the sides of the triangle, namely $OF = \\frac{EF}{2}$, and $OF = r = \\frac{\\sqrt{85}}{2}$.\n",
      "\n",
      "By the Pythagorean Theorem, $OQ^2 = OF^2 + OP^2$, which simplifies to $r^2 = \\left(\\frac{\\sqrt{85}}{2}\\right)^2 + OP^2$. Using the radius formula $r = 1.2668400381$, we can find that $OP^2 = 5.479342623$, thus $OP = 2.25$. Therefore, the bottom endpoint of the hypotenuse of triangle $DEF$ is $Q$, so $FQ = 2.25$. Rounded to the nearest integer, $FQ \\approx 2$.<|im_end|>\n",
      ">>> tensor(-0.1250)\n",
      ">>> tensor(0.)\n",
      ">>> <|im_start|>system\n",
      "\n",
      "You are a helpful assistant. The User asks a question, and the Assistant solves it. \n",
      "The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. And your final answer will be extracted automatically by the \\boxed{{}} tag.\n",
      "In your reasoning-process, You can use python-code to solve your problem. Put the code within ```python and ``` tags. The script will be executed immediately and output will be returned.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "Suppose that we have a right triangle $DEF$ with the right angle at $E$ such that $DF = \\sqrt{85}$ and $DE = 7$. A circle is drawn with its center on $DE$ such that the circle is tangent to $DF$ and $EF$. If $Q$ is the point where the circle and side $DF$ meet, then what is $FQ$?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "To find the length of $FQ$, we need to use the properties of similar triangles. The circle is tangent to $DF$ and $EF$, which implies that the angles at $D$ and $E$ are equal since they are subtended by the same arc.\n",
      "</think>\n",
      "<answer>\n",
      "Using similar triangles, we can establish the following ratio: \\(\\frac{FQ}{EQ} = \\frac{DF}{DE}\\). Given that \\(DF = \\sqrt{85}\\), \\(DE = 7\\), and \\(EQ = De - Fq\\), we can substitute and solve for \\(Fq\\).\n",
      "</answer>!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      ">>> tensor(-0.1250)\n",
      ">>> tensor(0.)\n",
      ">>> <|im_start|>system\n",
      "\n",
      "You are a helpful assistant. The User asks a question, and the Assistant solves it. \n",
      "The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. And your final answer will be extracted automatically by the \\boxed{{}} tag.\n",
      "In your reasoning-process, You can use python-code to solve your problem. Put the code within ```python and ``` tags. The script will be executed immediately and output will be returned.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "In a school, there are  $n$  students and some of them are friends each other. (Friendship is mutual.) Define  $ a, b $  the minimum value which satisfies the following conditions:\n",
      "(1) We can divide students into  $ a $  teams such that two students in the same team are always friends.\n",
      "(2) We can divide students into  $ b $  teams such that two students in the same team are never friends.\n",
      "Find the maximum value of  $ N = a+b $  in terms of  $n$ .<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "To find the maximum value of  $ N = a+b $ , we should first determine the minimum number of  $ a $ and  $ b $ for each team size. As we decrease team size, number of minimum teams are maximized.\n",
      "</think>\n",
      "<answer>\n",
      "```python\n",
      "def calculate_max_teams_teams():\n",
      "    max_teams = 0\n",
      "    for n in range(1, 100):\n",
      "        min_team = 2 * n\n",
      "        max_team = n - 1\n",
      "\n",
      "        if 2 * max_team < min_team:\n",
      "            min_team = 2 * max_team\n",
      "            max_team = n - 1\n",
      "\n",
      "        max_teams = max(max_teams, min_team + max_team)\n",
      "    return max_teams\n",
      "\n",
      "answer = calculate_max_teams_teams()\n",
      "print(answer)\n",
      "```\n",
      "```output\n",
      "294\n",
      "\n",
      "```\n",
      "The maximum value of $ N = a+b $ in terms of $n$ is $\\boxed{294}$.\n",
      "```<|im_end|>!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      ">>> tensor(-0.1250)\n",
      ">>> tensor(0.)\n",
      ">>> <|im_start|>system\n",
      "\n",
      "You are a helpful assistant. The User asks a question, and the Assistant solves it. \n",
      "The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. And your final answer will be extracted automatically by the \\boxed{{}} tag.\n",
      "In your reasoning-process, You can use python-code to solve your problem. Put the code within ```python and ``` tags. The script will be executed immediately and output will be returned.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "In a school, there are  $n$  students and some of them are friends each other. (Friendship is mutual.) Define  $ a, b $  the minimum value which satisfies the following conditions:\n",
      "(1) We can divide students into  $ a $  teams such that two students in the same team are always friends.\n",
      "(2) We can divide students into  $ b $  teams such that two students in the same team are never friends.\n",
      "Find the maximum value of  $ N = a+b $  in terms of  $n$ .<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "To be able to approach this, let's clearly maintain the friendship among students so that it does not pose a problem during mergers. This implies students should be placed into union types, which in turn, means to understand how many we need in a union type to maintain these friendships and cyclically distribute amongst teams.\n",
      "</think>\n",
      "<answer>\n",
      "Let us first realise that for each student, I could refer to them as their identity in the union type. Also, if there are n students, these identity numbers are distinct as they are just their id's and the same identity number cannot be observed across a range of union types (specifically teams combined, unions nested simultaneously), let's call these identities 1...n.\n",
      "Before diving into further relationship analysis let's also note that nU(nU(1+2+3...+ n)) <= (n-1) < n<p, so the result follows immediately; however note that the upper limit is strictly greater than n to guard against the possibility of subsets containing duplicates. Then for each loop over, update our nU(x) coordinate vector to contain all the union types we currently acknowledge, so their union type reflects their current attachments to friendlies within union type x.\n",
      "</answer>!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      ">>> tensor(-0.1250)\n",
      ">>> tensor(0.)\n",
      ">>> <|im_start|>system\n",
      "\n",
      "You are a helpful assistant. The User asks a question, and the Assistant solves it. \n",
      "The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. And your final answer will be extracted automatically by the \\boxed{{}} tag.\n",
      "In your reasoning-process, You can use python-code to solve your problem. Put the code within ```python and ``` tags. The script will be executed immediately and output will be returned.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "In a school, there are  $n$  students and some of them are friends each other. (Friendship is mutual.) Define  $ a, b $  the minimum value which satisfies the following conditions:\n",
      "(1) We can divide students into  $ a $  teams such that two students in the same team are always friends.\n",
      "(2) We can divide students into  $ b $  teams such that two students in the same team are never friends.\n",
      "Find the maximum value of  $ N = a+b $  in terms of  $n$ .<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "n students are in a school. To solve the problem, we will use recursion.\n",
      "</think>\n",
      "\n",
      "```python\n",
      "def fn(n):\n",
      "    if n == 0:\n",
      "        return 1\n",
      "    else:\n",
      "        # Recursion Base Case\n",
      "        if n == 1:\n",
      "            return 1\n",
      "\n",
      "        # Recursive Case\n",
      "        first_team = 1 + fn(n - 1)\n",
      "\n",
      "        second_team = 1 + fn(n - 2)\n",
      "\n",
      "        # Total Sum\n",
      "        total = first_team + second_team\n",
      "\n",
      "        return total\n",
      "\n",
      "n = int(input())\n",
      "\n",
      "print(fn(n))\n",
      "```\n",
      "\n",
      "```output\n",
      "Traceback (most recent call last):\n",
      "   line 20, in <module>\n",
      "EOFError: EOF when reading a line\n",
      "\n",
      "```\n",
      "<answer>\n",
      "1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 + ... + n-1\n",
      "</answer>!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      ">>> tensor(-0.1250)\n",
      ">>> tensor(0.)\n",
      ">>> <|im_start|>system\n",
      "\n",
      "You are a helpful assistant. The User asks a question, and the Assistant solves it. \n",
      "The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. And your final answer will be extracted automatically by the \\boxed{{}} tag.\n",
      "In your reasoning-process, You can use python-code to solve your problem. Put the code within ```python and ``` tags. The script will be executed immediately and output will be returned.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "In a school, there are  $n$  students and some of them are friends each other. (Friendship is mutual.) Define  $ a, b $  the minimum value which satisfies the following conditions:\n",
      "(1) We can divide students into  $ a $  teams such that two students in the same team are always friends.\n",
      "(2) We can divide students into  $ b $  teams such that two students in the same team are never friends.\n",
      "Find the maximum value of  $ N = a+b $  in terms of  $n$ .<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "To maximize the value of $N = a+b$, we need to maximize $a$ and $b$ subject to the given conditions. Let's assume there are $p$ pairs of friends among the $n$ students, where $p$ is unknown for now and $0 < p \\leq n$. We would like to maximize $a$ by distributing $p$ pairs into two teams of $a$ and $(n-p)$ students respectively, such that $a > 2(n-p)$ (one team of $a$ students does not have to have all students be friends, and the other has $2(n-p)$ students and thus does not have to have any friends). This constraint will remain to establish the minimum of $a$ and $b$ as we're considering teams formed from these pairs and their distribution. Similar considerations apply to $b$.\n",
      "\n",
      "Next, we should try to distribute the remaining students into teams of $b$. This means suitable distribution of teams of size $b$, so it should maximally maintain the requirement. To better quantify this variable, it should count how many students are left to be divided.\n",
      "\n",
      "Let's check where the max value arises.\n",
      "</think>\n",
      "\n",
      "<answer>\n",
      "\n",
      "For $a$, we have two constraints: $a > 2(n-p)$ and $1 \\le a \\le 2(n-p)$.\n",
      "For $b$, we have two constraints: $b \\le \\left\\lceil \\frac{n-p}{2} \\right\\rceil$ and $b \\ge 1$.\n",
      "The sum $a+b$ is maximized when $a=2(n-p)$ and $b=\\left\\lceil \\frac{n-p}{2} \\right\\rceil - 1$.\n",
      "\n",
      "Therefore, the maximum value of $N = a+b$ in terms of $n$ is $2n$.\n",
      "</answer>!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      ">>> tensor(-0.1250)\n",
      ">>> tensor(0.)\n",
      ">>> <|im_start|>system\n",
      "\n",
      "You are a helpful assistant. The User asks a question, and the Assistant solves it. \n",
      "The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. And your final answer will be extracted automatically by the \\boxed{{}} tag.\n",
      "In your reasoning-process, You can use python-code to solve your problem. Put the code within ```python and ``` tags. The script will be executed immediately and output will be returned.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "In a school, there are  $n$  students and some of them are friends each other. (Friendship is mutual.) Define  $ a, b $  the minimum value which satisfies the following conditions:\n",
      "(1) We can divide students into  $ a $  teams such that two students in the same team are always friends.\n",
      "(2) We can divide students into  $ b $  teams such that two students in the same team are never friends.\n",
      "Find the maximum value of  $ N = a+b $  in terms of  $n$ .<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "The problem asks for a maximum value of $N$ in terms of $n$. We can start by finding the number of mutual friends and mutual enemies in the team setup from (1) and (2). A team has exactly $4$ mutual friends and $4$ mutual enemies. So in $a+1$ teams of 4, there will be $4a$ mutual friends and $4$. So in $b+1$ teams, the number of the mutual friends is $4b$, which is equal to $a+2$ we subtract 2 because the last team we create must be mostly enemies. Therefore $4b = a + 2$. The maximum value of $a$ is when we have a team of 4 mutual enemies. So $N = a + (b+1)$ where $N = 4b + (a+1)$ and $ a <= b$ because minimum value of stranger ever is 2. Solving the above equation, we get $b^2 - 2b + 3 = 0$ which has no integer solutions. But in the real numbers, there is no additional $b$ smaller than $b+1$ so we don't need to worry too much but trying to find other equations to solve but nothing so far.\n",
      "</think> <answer> $12$ </answer>!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      ">>> tensor(-0.1250)\n",
      ">>> tensor(0.)\n",
      ">>> <|im_start|>system\n",
      "\n",
      "You are a helpful assistant. The User asks a question, and the Assistant solves it. \n",
      "The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. And your final answer will be extracted automatically by the \\boxed{{}} tag.\n",
      "In your reasoning-process, You can use python-code to solve your problem. Put the code within ```python and ``` tags. The script will be executed immediately and output will be returned.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "In a school, there are  $n$  students and some of them are friends each other. (Friendship is mutual.) Define  $ a, b $  the minimum value which satisfies the following conditions:\n",
      "(1) We can divide students into  $ a $  teams such that two students in the same team are always friends.\n",
      "(2) We can divide students into  $ b $  teams such that two students in the same team are never friends.\n",
      "Find the maximum value of  $ N = a+b $  in terms of  $n$ .<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "To solve this problem, we need to find the relationship between 'n' and the values of a and b. We can use a Python sympy library code to iterate through values of n and find the maximum of N.\n",
      "\n",
      "Also symmetric to this problem, we can try to formulate a relationship between a+b and n and depend on n we can formulate the constraint.\n",
      "\n",
      "Where A and B are permutation-related, n stands for number of students. Suppose n=5 students. We search for b=2 and A=1, this works well, where (b+1)*(a-1) <= n-1.\n",
      "Searching with more actual results we get (14, 6). So our tilde values here and the computed values for =~ 13, 6. Last results we suppose that 6 and 13 are stuck in the limit of our hardware speed.\n",
      "</think>\n",
      "<answer>\n",
      "The maximum value of $N = a+b$ in terms of $n$ is 13.\n",
      "```\n",
      "```<|im_end|>!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      ">>> tensor(-0.1250)\n",
      ">>> tensor(0.)\n",
      ">>> <|im_start|>system\n",
      "\n",
      "You are a helpful assistant. The User asks a question, and the Assistant solves it. \n",
      "The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. And your final answer will be extracted automatically by the \\boxed{{}} tag.\n",
      "In your reasoning-process, You can use python-code to solve your problem. Put the code within ```python and ``` tags. The script will be executed immediately and output will be returned.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "In a school, there are  $n$  students and some of them are friends each other. (Friendship is mutual.) Define  $ a, b $  the minimum value which satisfies the following conditions:\n",
      "(1) We can divide students into  $ a $  teams such that two students in the same team are always friends.\n",
      "(2) We can divide students into  $ b $  teams such that two students in the same team are never friends.\n",
      "Find the maximum value of  $ N = a+b $  in terms of  $n$ .<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "We can model this question as a maximum value of an LP. We'll have two variables $a, b$ where $a$ accounts for the friendships in the smallest team and $b$ accounts for the friendships in the largest team, and $N=a+b$ represents the number of teams formed.\n",
      "</think>\n",
      "<answer>\n",
      "```python\n",
      "from ortools.linear_solver import pywraplp\n",
      "\n",
      "lp = pywraplp.Solver(\"MaximizeN problem\", pywraplp.Solver.CBC_MIXED_INTEGER_PROGRAMMING)\n",
      "\n",
      "a, b = lp.NumVar(0, n, \"a\"), lp.NumVar(0, n, \"b\")\n",
      "\n",
      "N = a + b\n",
      "lp.OptimizationLimit = 1e6   # arbitrary large number which may cause integer feasibility issues\n",
      "lp.IsMaximization = True\n",
      "lp.Minimize(N)\n",
      "\n",
      "lp.Add(a <= n).Start()\n",
      "lp.Add(b <= n).End()\n",
      "\n",
      "sol = lp.Solve()\n",
      "\n",
      "print(sol.ObjectiveValue())      # a+b as a fractional value\n",
      "lp.GetSolver().Status()\n",
      "```\n",
      "\n",
      "```output\n",
      "Traceback (most recent call last):\n",
      "   line 2, in <module>\n",
      "ModuleNotFoundError: No module named 'ortools'\n",
      "\n",
      "```\n",
      "The function to solve the LP problem with integer constraints is not available in the `pywraplp.shapes` module. To overcome this limitation, it appears that the value of $N$ is dependent on $n$ and does not fit into a straightforward characterization like a maximum or minimum value. Instead, the maximum value of $N$ is determined by the number of students in a school, $n$, and how well these students can be arranged into teams of mutual friends or enemies.\n",
      "\n",
      "Given the complexity of this problem involving multiple constraints and variables, it would be optimal to rotate this question to a more accurately representable form or use a more suited technique for the specific constraint-solving algorithms, which might include using a dedicated solver designed for integer programming, beyond basic LP methods or extending with a less commonly used solver.\n",
      "```<|im_end|>!!!!!!!!!!!!!!!!!!!!\n",
      ">>> tensor(-0.1250)\n",
      ">>> tensor(0.)\n",
      ">>> <|im_start|>system\n",
      "\n",
      "You are a helpful assistant. The User asks a question, and the Assistant solves it. \n",
      "The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. And your final answer will be extracted automatically by the \\boxed{{}} tag.\n",
      "In your reasoning-process, You can use python-code to solve your problem. Put the code within ```python and ``` tags. The script will be executed immediately and output will be returned.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "In a school, there are  $n$  students and some of them are friends each other. (Friendship is mutual.) Define  $ a, b $  the minimum value which satisfies the following conditions:\n",
      "(1) We can divide students into  $ a $  teams such that two students in the same team are always friends.\n",
      "(2) We can divide students into  $ b $  teams such that two students in the same team are never friends.\n",
      "Find the maximum value of  $ N = a+b $  in terms of  $n$ .<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "To solve this, note that <b>(1) implies (2); and (2) implies (1). Thus, the condition is necessary and sufficient.</b> Let there be <i>n</i> students. Consider our lattice of students, <b>with one void cell: the hypothetical student.</b> By pigeonhole among the <i>n</i> cells, then at least of $ n + 1 $ cells is occupied, just one being the hypothetical location, followed by a student. This student is connected to all other as friends, so after replacing them all we are back at the maximum of <i>a</i> above. Plus one finally to account for the hypothetical student which is not connected.\n",
      "</think>\n",
      "<answer> max <i>a + b</i> = $n + 1$</answer>!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      ">>> tensor(0.8750)\n",
      ">>> tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(debug_batch)):\n",
    "    print(\">>>\", tokenizer.decode(debug_batch[i]['input_ids']))\n",
    "    print(\">>>\", debug_batch[i]['rewards'])\n",
    "    print(\">>>\", debug_batch[i]['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b77d3fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 151644 token: <|im_start|>                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 8948 token: system                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 271 token: \n",
      "\n",
      "                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 2610 token: You                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 525 token:  are                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 264 token:  a                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 10950 token:  helpful                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 17847 token:  assistant                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 13 token: .                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 576 token:  The                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 2657 token:  User                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 17064 token:  asks                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 264 token:  a                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 3405 token:  question                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 11 token: ,                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 323 token:  and                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 279 token:  the                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 21388 token:  Assistant                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 67477 token:  solves                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 432 token:  it                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 13 token: .                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 715 token:  \n",
      "                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 785 token: The                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 21388 token:  Assistant                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1156 token:  first                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 15482 token:  thinks                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 911 token:  about                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 279 token:  the                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 32711 token:  reasoning                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1882 token:  process                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 304 token:  in                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 279 token:  the                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 3971 token:  mind                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 323 token:  and                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1221 token:  then                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 5707 token:  provides                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 279 token:  the                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 2657 token:  User                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 448 token:  with                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 279 token:  the                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 4226 token:  answer                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 13 token: .                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 576 token:  The                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 32711 token:  reasoning                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1882 token:  process                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 374 token:  is                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 43810 token:  enclosed                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 2878 token:  within                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 366 token:  <                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 26865 token: think                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 29 token: >                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 690 token:  </                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 26865 token: think                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 29 token: >                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 323 token:  and                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 4226 token:  answer                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 374 token:  is                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 43810 token:  enclosed                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 2878 token:  within                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 366 token:  <                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 9217 token: answer                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 29 token: >                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 690 token:  </                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 9217 token: answer                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 29 token: >                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 9492 token:  tags                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 11 token: ,                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 15576 token:  respectively                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 11 token: ,                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 600 token:  i                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1734 token: .e                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 2572 token: .,                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 366 token:  <                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 26865 token: think                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 29 token: >                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 32711 token:  reasoning                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1882 token:  process                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1588 token:  here                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 690 token:  </                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 26865 token: think                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 29 token: >                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 366 token:  <                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 9217 token: answer                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 29 token: >                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 4226 token:  answer                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1588 token:  here                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 690 token:  </                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 9217 token: answer                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 14276 token: >.                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1597 token:  And                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 697 token:  your                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1590 token:  final                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 4226 token:  answer                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 686 token:  will                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 387 token:  be                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 27432 token:  extracted                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 9463 token:  automatically                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 553 token:  by                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 279 token:  the                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1124 token:  \\                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 79075 token: boxed                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 2979 token: {{                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 3417 token: }}                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 4772 token:  tag                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 624 token: .\n",
      "                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 641 token: In                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 697 token:  your                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 32711 token:  reasoning                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 50094 token: -process                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 11 token: ,                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1446 token:  You                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 646 token:  can                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 990 token:  use                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 10135 token:  python                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 25261 token: -code                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 311 token:  to                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 11625 token:  solve                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 697 token:  your                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 3491 token:  problem                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 13 token: .                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 10224 token:  Put                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 279 token:  the                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 2038 token:  code                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 2878 token:  within                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 54275 token:  ```                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 12669 token: python                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 323 token:  and                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 54275 token:  ```                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 9492 token:  tags                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 13 token: .                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 576 token:  The                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 5316 token:  script                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 686 token:  will                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 387 token:  be                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 15695 token:  executed                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 7069 token:  immediately                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 323 token:  and                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 2550 token:  output                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 686 token:  will                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 387 token:  be                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 5927 token:  returned                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 624 token: .\n",
      "                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 151645 token: <|im_end|>                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 198 token: \n",
      "                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 151644 token: <|im_start|>                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 872 token: user                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 198 token: \n",
      "                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 10048 token: Sup                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 2900 token: pose                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 429 token:  that                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 582 token:  we                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 614 token:  have                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 264 token:  a                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1290 token:  right                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 21495 token:  triangle                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 400 token:  $                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 13649 token: DEF                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 3 token: $                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 448 token:  with                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 279 token:  the                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1290 token:  right                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 9210 token:  angle                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 518 token:  at                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 400 token:  $                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 36 token: E                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 3 token: $                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1741 token:  such                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 429 token:  that                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 400 token:  $                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 5262 token: DF                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 284 token:  =                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1124 token:  \\                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 26888 token: sqrt                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 90 token: {                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 23 token: 8                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 20 token: 5                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 31716 token: }$                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 323 token:  and                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 400 token:  $                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1150 token: DE                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 284 token:  =                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 220 token:                    logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 22 token: 7                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 12947 token: $.                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 362 token:  A                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 12671 token:  circle                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 374 token:  is                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 14764 token:  drawn                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 448 token:  with                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1181 token:  its                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 4126 token:  center                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 389 token:  on                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 400 token:  $                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1150 token: DE                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 3 token: $                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1741 token:  such                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 429 token:  that                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 279 token:  the                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 12671 token:  circle                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 374 token:  is                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 68660 token:  tangent                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 311 token:  to                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 400 token:  $                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 5262 token: DF                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 3 token: $                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 323 token:  and                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 400 token:  $                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 14884 token: EF                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 12947 token: $.                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1416 token:  If                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 400 token:  $                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 48 token: Q                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 3 token: $                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 374 token:  is                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 279 token:  the                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1459 token:  point                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1380 token:  where                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 279 token:  the                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 12671 token:  circle                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 323 token:  and                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 3108 token:  side                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 400 token:  $                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 5262 token: DF                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 3 token: $                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 3367 token:  meet                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 11 token: ,                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1221 token:  then                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1128 token:  what                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 374 token:  is                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 400 token:  $                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 37 token: F                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 48 token: Q                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 3 token: $                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 30 token: ?                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 151645 token: <|im_end|>                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 198 token: \n",
      "                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 151644 token: <|im_start|>                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 77091 token: assistant                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 198 token: \n",
      "                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 13708 token: <th                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 766 token: ink                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 397 token: >\n",
      "                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1654 token: We                  logprob: -2.0899 loss_mask: True attention_mask: True\n",
      "id: 646 token:  can                  logprob: -0.4286 loss_mask: True attention_mask: True\n",
      "id: 990 token:  use                  logprob: -0.4337 loss_mask: True attention_mask: True\n",
      "id: 279 token:  the                  logprob: -0.1921 loss_mask: True attention_mask: True\n",
      "id: 5355 token:  Py                  logprob: -0.3049 loss_mask: True attention_mask: True\n",
      "id: 95362 token: thag                  logprob: -0.0001 loss_mask: True attention_mask: True\n",
      "id: 45195 token: orean                  logprob: -0.0026 loss_mask: True attention_mask: True\n",
      "id: 576 token:  The                  logprob: -1.2280 loss_mask: True attention_mask: True\n",
      "id: 13173 token: orem                  logprob: -0.0000 loss_mask: True attention_mask: True\n",
      "id: 311 token:  to                  logprob: -0.1413 loss_mask: True attention_mask: True\n",
      "id: 1477 token:  find                  logprob: -0.0886 loss_mask: True attention_mask: True\n",
      "id: 279 token:  the                  logprob: -0.0738 loss_mask: True attention_mask: True\n",
      "id: 3084 token:  length                  logprob: -0.0130 loss_mask: True attention_mask: True\n",
      "id: 315 token:  of                  logprob: -0.0001 loss_mask: True attention_mask: True\n",
      "id: 400 token:  $                  logprob: -0.4130 loss_mask: True attention_mask: True\n",
      "id: 14884 token: EF                  logprob: -0.0658 loss_mask: True attention_mask: True\n",
      "id: 12947 token: $.                  logprob: -0.4575 loss_mask: True attention_mask: True\n",
      "id: 5005 token:  Then                  logprob: -0.7251 loss_mask: True attention_mask: True\n",
      "id: 11 token: ,                  logprob: -0.3215 loss_mask: True attention_mask: True\n",
      "id: 582 token:  we                  logprob: -0.3538 loss_mask: True attention_mask: True\n",
      "id: 646 token:  can                  logprob: -0.0413 loss_mask: True attention_mask: True\n",
      "id: 990 token:  use                  logprob: -0.1566 loss_mask: True attention_mask: True\n",
      "id: 4428 token:  similar                  logprob: -1.3513 loss_mask: True attention_mask: True\n",
      "id: 42446 token:  triangles                  logprob: -0.0042 loss_mask: True attention_mask: True\n",
      "id: 311 token:  to                  logprob: -0.0161 loss_mask: True attention_mask: True\n",
      "id: 1477 token:  find                  logprob: -0.0330 loss_mask: True attention_mask: True\n",
      "id: 279 token:  the                  logprob: -0.1400 loss_mask: True attention_mask: True\n",
      "id: 3084 token:  length                  logprob: -0.4595 loss_mask: True attention_mask: True\n",
      "id: 315 token:  of                  logprob: -0.0006 loss_mask: True attention_mask: True\n",
      "id: 400 token:  $                  logprob: -0.0036 loss_mask: True attention_mask: True\n",
      "id: 37 token: F                  logprob: -0.0731 loss_mask: True attention_mask: True\n",
      "id: 48 token: Q                  logprob: -0.0000 loss_mask: True attention_mask: True\n",
      "id: 3 token: $                  logprob: -0.5270 loss_mask: True attention_mask: True\n",
      "id: 624 token: .\n",
      "                  logprob: -0.0225 loss_mask: True attention_mask: True\n",
      "id: 522 token: </                  logprob: -0.0290 loss_mask: True attention_mask: True\n",
      "id: 26865 token: think                  logprob: -0.0003 loss_mask: True attention_mask: True\n",
      "id: 1339 token: >\n",
      "\n",
      "                  logprob: -0.5902 loss_mask: True attention_mask: True\n",
      "id: 27 token: <                  logprob: -0.0063 loss_mask: True attention_mask: True\n",
      "id: 9217 token: answer                  logprob: -0.0002 loss_mask: True attention_mask: True\n",
      "id: 397 token: >\n",
      "                  logprob: -0.0651 loss_mask: True attention_mask: True\n",
      "id: 73594 token: ```                  logprob: -0.8311 loss_mask: True attention_mask: True\n",
      "id: 12669 token: python                  logprob: -0.0086 loss_mask: True attention_mask: True\n",
      "id: 198 token: \n",
      "                  logprob: -0.0016 loss_mask: True attention_mask: True\n",
      "id: 1499 token: from                  logprob: -0.3635 loss_mask: True attention_mask: True\n",
      "id: 21459 token:  symp                  logprob: -0.4297 loss_mask: True attention_mask: True\n",
      "id: 88 token: y                  logprob: -0.0000 loss_mask: True attention_mask: True\n",
      "id: 1159 token:  import                  logprob: -0.0002 loss_mask: True attention_mask: True\n",
      "id: 17738 token:  symbols                  logprob: -2.2605 loss_mask: True attention_mask: True\n",
      "id: 11 token: ,                  logprob: -0.0042 loss_mask: True attention_mask: True\n",
      "id: 17946 token:  sqrt                  logprob: -1.1244 loss_mask: True attention_mask: True\n",
      "id: 11 token: ,                  logprob: -1.0571 loss_mask: True attention_mask: True\n",
      "id: 33122 token:  Eq                  logprob: -1.5704 loss_mask: True attention_mask: True\n",
      "id: 11 token: ,                  logprob: -0.0130 loss_mask: True attention_mask: True\n",
      "id: 11625 token:  solve                  logprob: -0.0015 loss_mask: True attention_mask: True\n",
      "id: 271 token: \n",
      "\n",
      "                  logprob: -0.0149 loss_mask: True attention_mask: True\n",
      "id: 5262 token: DF                  logprob: -2.3972 loss_mask: True attention_mask: True\n",
      "id: 11 token: ,                  logprob: -2.7149 loss_mask: True attention_mask: True\n",
      "id: 3385 token:  DE                  logprob: -0.1872 loss_mask: True attention_mask: True\n",
      "id: 284 token:  =                  logprob: -0.9744 loss_mask: True attention_mask: True\n",
      "id: 17946 token:  sqrt                  logprob: -0.9419 loss_mask: True attention_mask: True\n",
      "id: 7 token: (                  logprob: -0.0001 loss_mask: True attention_mask: True\n",
      "id: 23 token: 8                  logprob: -0.0001 loss_mask: True attention_mask: True\n",
      "id: 20 token: 5                  logprob: -0.0001 loss_mask: True attention_mask: True\n",
      "id: 701 token: ),                  logprob: -0.0005 loss_mask: True attention_mask: True\n",
      "id: 220 token:                    logprob: -0.0013 loss_mask: True attention_mask: True\n",
      "id: 22 token: 7                  logprob: -0.0002 loss_mask: True attention_mask: True\n",
      "id: 220 token:                    logprob: -3.3585 loss_mask: True attention_mask: True\n",
      "id: 671 token:  #                  logprob: -0.0001 loss_mask: True attention_mask: True\n",
      "id: 274 token:  s                  logprob: -7.8667 loss_mask: True attention_mask: True\n",
      "id: 3341 token: ides                  logprob: -0.9750 loss_mask: True attention_mask: True\n",
      "id: 315 token:  of                  logprob: -0.0748 loss_mask: True attention_mask: True\n",
      "id: 279 token:  the                  logprob: -0.3046 loss_mask: True attention_mask: True\n",
      "id: 1290 token:  right                  logprob: -0.2041 loss_mask: True attention_mask: True\n",
      "id: 21495 token:  triangle                  logprob: -0.0069 loss_mask: True attention_mask: True\n",
      "id: 23865 token:  DEF                  logprob: -0.9003 loss_mask: True attention_mask: True\n",
      "id: 304 token:  in                  logprob: -5.7430 loss_mask: True attention_mask: True\n",
      "id: 264 token:  a                  logprob: -3.4594 loss_mask: True attention_mask: True\n",
      "id: 5355 token:  Py                  logprob: -1.0183 loss_mask: True attention_mask: True\n",
      "id: 95362 token: thag                  logprob: -0.0002 loss_mask: True attention_mask: True\n",
      "id: 45195 token: orean                  logprob: -0.0036 loss_mask: True attention_mask: True\n",
      "id: 23725 token:  triple                  logprob: -0.2451 loss_mask: True attention_mask: True\n",
      "id: 315 token:  of                  logprob: -4.2382 loss_mask: True attention_mask: True\n",
      "id: 17946 token:  sqrt                  logprob: -2.8648 loss_mask: True attention_mask: True\n",
      "id: 7 token: (                  logprob: -0.0245 loss_mask: True attention_mask: True\n",
      "id: 23 token: 8                  logprob: -0.0424 loss_mask: True attention_mask: True\n",
      "id: 20 token: 5                  logprob: -0.0019 loss_mask: True attention_mask: True\n",
      "id: 701 token: ),                  logprob: -0.3437 loss_mask: True attention_mask: True\n",
      "id: 220 token:                    logprob: -0.0854 loss_mask: True attention_mask: True\n",
      "id: 22 token: 7                  logprob: -0.0226 loss_mask: True attention_mask: True\n",
      "id: 11 token: ,                  logprob: -0.2409 loss_mask: True attention_mask: True\n",
      "id: 220 token:                    logprob: -0.3174 loss_mask: True attention_mask: True\n",
      "id: 23 token: 8                  logprob: -2.2858 loss_mask: True attention_mask: True\n",
      "id: 198 token: \n",
      "                  logprob: -0.9985 loss_mask: True attention_mask: True\n",
      "id: 14884 token: EF                  logprob: -0.1251 loss_mask: True attention_mask: True\n",
      "id: 284 token:  =                  logprob: -0.0176 loss_mask: True attention_mask: True\n",
      "id: 17738 token:  symbols                  logprob: -2.0641 loss_mask: True attention_mask: True\n",
      "id: 492 token: ('                  logprob: -0.0011 loss_mask: True attention_mask: True\n",
      "id: 14884 token: EF                  logprob: -0.0028 loss_mask: True attention_mask: True\n",
      "id: 4610 token: ')\n",
      "\n",
      "                  logprob: -0.6859 loss_mask: True attention_mask: True\n",
      "id: 2 token: #                  logprob: -0.0516 loss_mask: True attention_mask: True\n",
      "id: 12091 token:  Using                  logprob: -1.7447 loss_mask: True attention_mask: True\n",
      "id: 279 token:  the                  logprob: -0.4384 loss_mask: True attention_mask: True\n",
      "id: 5355 token:  Py                  logprob: -0.0131 loss_mask: True attention_mask: True\n",
      "id: 95362 token: thag                  logprob: -0.0000 loss_mask: True attention_mask: True\n",
      "id: 45195 token: orean                  logprob: -0.0001 loss_mask: True attention_mask: True\n",
      "id: 576 token:  The                  logprob: -0.1293 loss_mask: True attention_mask: True\n",
      "id: 13173 token: orem                  logprob: -0.0000 loss_mask: True attention_mask: True\n",
      "id: 198 token: \n",
      "                  logprob: -1.7779 loss_mask: True attention_mask: True\n",
      "id: 11006 token: eq                  logprob: -0.3012 loss_mask: True attention_mask: True\n",
      "id: 16 token: 1                  logprob: -0.7035 loss_mask: True attention_mask: True\n",
      "id: 284 token:  =                  logprob: -0.0002 loss_mask: True attention_mask: True\n",
      "id: 33122 token:  Eq                  logprob: -0.0003 loss_mask: True attention_mask: True\n",
      "id: 7 token: (                  logprob: -0.0376 loss_mask: True attention_mask: True\n",
      "id: 5262 token: DF                  logprob: -0.0042 loss_mask: True attention_mask: True\n",
      "id: 334 token: **                  logprob: -0.0053 loss_mask: True attention_mask: True\n",
      "id: 17 token: 2                  logprob: -0.0001 loss_mask: True attention_mask: True\n",
      "id: 488 token:  +                  logprob: -0.3274 loss_mask: True attention_mask: True\n",
      "id: 3385 token:  DE                  logprob: -0.4832 loss_mask: True attention_mask: True\n",
      "id: 334 token: **                  logprob: -0.0001 loss_mask: True attention_mask: True\n",
      "id: 17 token: 2                  logprob: -0.0000 loss_mask: True attention_mask: True\n",
      "id: 11 token: ,                  logprob: -0.0041 loss_mask: True attention_mask: True\n",
      "id: 43990 token:  EF                  logprob: -0.0296 loss_mask: True attention_mask: True\n",
      "id: 334 token: **                  logprob: -0.0002 loss_mask: True attention_mask: True\n",
      "id: 17 token: 2                  logprob: -0.0000 loss_mask: True attention_mask: True\n",
      "id: 692 token: )\n",
      "\n",
      "                  logprob: -0.2975 loss_mask: True attention_mask: True\n",
      "id: 2 token: #                  logprob: -0.0067 loss_mask: True attention_mask: True\n",
      "id: 63284 token:  Solve                  logprob: -0.3130 loss_mask: True attention_mask: True\n",
      "id: 369 token:  for                  logprob: -0.0400 loss_mask: True attention_mask: True\n",
      "id: 43990 token:  EF                  logprob: -0.0054 loss_mask: True attention_mask: True\n",
      "id: 198 token: \n",
      "                  logprob: -0.0488 loss_mask: True attention_mask: True\n",
      "id: 14884 token: EF                  logprob: -0.3376 loss_mask: True attention_mask: True\n",
      "id: 50274 token: _solution                  logprob: -0.4280 loss_mask: True attention_mask: True\n",
      "id: 284 token:  =                  logprob: -0.0004 loss_mask: True attention_mask: True\n",
      "id: 11625 token:  solve                  logprob: -0.0004 loss_mask: True attention_mask: True\n",
      "id: 69909 token: (eq                  logprob: -0.0010 loss_mask: True attention_mask: True\n",
      "id: 16 token: 1                  logprob: -0.0000 loss_mask: True attention_mask: True\n",
      "id: 11 token: ,                  logprob: -0.0075 loss_mask: True attention_mask: True\n",
      "id: 43990 token:  EF                  logprob: -0.0007 loss_mask: True attention_mask: True\n",
      "id: 692 token: )\n",
      "\n",
      "                  logprob: -0.6830 loss_mask: True attention_mask: True\n",
      "id: 14884 token: EF                  logprob: -2.2545 loss_mask: True attention_mask: True\n",
      "id: 50274 token: _solution                  logprob: -0.2039 loss_mask: True attention_mask: True\n",
      "id: 58 token: [                  logprob: -0.7470 loss_mask: True attention_mask: True\n",
      "id: 15 token: 0                  logprob: -0.0382 loss_mask: True attention_mask: True\n",
      "id: 921 token: ]\n",
      "                  logprob: -0.5316 loss_mask: True attention_mask: True\n",
      "id: 13874 token: ``                  logprob: -0.0226 loss_mask: True attention_mask: True\n",
      "id: 19324 token: `\n",
      "\n",
      "                  logprob: -3.4086 loss_mask: True attention_mask: True\n",
      "id: 198 token: \n",
      "                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 73594 token: ```                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 3006 token: output                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 271 token: \n",
      "\n",
      "                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 13874 token: ``                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 3989 token: `\n",
      "                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 2753 token: No                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 2550 token:  output                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 13 token: .                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 6771 token:  Let                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 752 token:  me                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 990 token:  use                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1565 token:  `                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1350 token: print                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 54258 token: ()`                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 311 token:  to                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1490 token:  see                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 279 token:  the                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1102 token:  result                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 323 token:  and                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1430 token:  try                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1549 token:  again                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 624 token: .\n",
      "                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 73594 token: ```                  logprob: -0.8747 loss_mask: True attention_mask: True\n",
      "id: 12669 token: python                  logprob: -2.0334 loss_mask: True attention_mask: True\n",
      "id: 198 token: \n",
      "                  logprob: -0.3249 loss_mask: True attention_mask: True\n",
      "id: 73594 token: ```                  logprob: -2.0681 loss_mask: True attention_mask: True\n",
      "id: 198 token: \n",
      "                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 73594 token: ```                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 3006 token: output                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 271 token: \n",
      "\n",
      "                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 13874 token: ``                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 3989 token: `\n",
      "                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 2753 token: No                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 2550 token:  output                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 13 token: .                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 6771 token:  Let                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 752 token:  me                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 990 token:  use                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1565 token:  `                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1350 token: print                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 54258 token: ()`                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 311 token:  to                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1490 token:  see                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 279 token:  the                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1102 token:  result                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 323 token:  and                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1430 token:  try                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1549 token:  again                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 624 token: .\n",
      "                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 73594 token: ```                  logprob: -0.2544 loss_mask: True attention_mask: True\n",
      "id: 12669 token: python                  logprob: -0.0198 loss_mask: True attention_mask: True\n",
      "id: 198 token: \n",
      "                  logprob: -0.1052 loss_mask: True attention_mask: True\n",
      "id: 73594 token: ```                  logprob: -0.1278 loss_mask: True attention_mask: True\n",
      "id: 198 token: \n",
      "                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 73594 token: ```                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 3006 token: output                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 271 token: \n",
      "\n",
      "                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 13874 token: ``                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 3989 token: `\n",
      "                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 2753 token: No                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 2550 token:  output                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 13 token: .                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 6771 token:  Let                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 752 token:  me                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 990 token:  use                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1565 token:  `                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1350 token: print                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 54258 token: ()`                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 311 token:  to                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1490 token:  see                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 279 token:  the                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1102 token:  result                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 323 token:  and                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1430 token:  try                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1549 token:  again                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 624 token: .\n",
      "                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 73594 token: ```                  logprob: -0.0498 loss_mask: True attention_mask: True\n",
      "id: 12669 token: python                  logprob: -0.0080 loss_mask: True attention_mask: True\n",
      "id: 271 token: \n",
      "\n",
      "                  logprob: -3.7748 loss_mask: True attention_mask: True\n",
      "id: 13874 token: ``                  logprob: -2.8876 loss_mask: True attention_mask: True\n",
      "id: 19324 token: `\n",
      "\n",
      "                  logprob: -1.2237 loss_mask: True attention_mask: True\n",
      "id: 198 token: \n",
      "                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 73594 token: ```                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 3006 token: output                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 271 token: \n",
      "\n",
      "                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 13874 token: ``                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 3989 token: `\n",
      "                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 2753 token: No                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 2550 token:  output                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 13 token: .                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 6771 token:  Let                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 752 token:  me                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 990 token:  use                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1565 token:  `                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1350 token: print                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 54258 token: ()`                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 311 token:  to                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1490 token:  see                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 279 token:  the                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1102 token:  result                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 323 token:  and                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1430 token:  try                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 1549 token:  again                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 624 token: .\n",
      "                  logprob: 0.0000 loss_mask: False attention_mask: True\n",
      "id: 13874 token: ``                  logprob: -2.8149 loss_mask: True attention_mask: True\n",
      "id: 19324 token: `\n",
      "\n",
      "                  logprob: -0.0553 loss_mask: True attention_mask: True\n",
      "id: 73594 token: ```                  logprob: -1.1541 loss_mask: True attention_mask: True\n",
      "id: 52721 token: execution                  logprob: -5.8645 loss_mask: True attention_mask: True\n",
      "id: 5287 token: _result                  logprob: -1.3765 loss_mask: True attention_mask: True\n",
      "id: 271 token: \n",
      "\n",
      "                  logprob: -0.6948 loss_mask: True attention_mask: True\n",
      "id: 73594 token: ```                  logprob: -0.7721 loss_mask: True attention_mask: True\n",
      "id: 151645 token: <|im_end|>                  logprob: -0.7174 loss_mask: True attention_mask: True\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n",
      "id: 0 token: !                  logprob: 0.0000 loss_mask: False attention_mask: False\n"
     ]
    }
   ],
   "source": [
    "idx = 1\n",
    "for i in range(len(debug_batch[idx]['input_ids'])):\n",
    "    print(f\"id: {debug_batch[idx]['input_ids'][i]} token: {tokenizer.decode(debug_batch[idx]['input_ids'][i])}                  logprob: {debug_batch[idx]['logprobs'][i]:.4f} loss_mask: {debug_batch[idx]['loss_mask'][i]} attention_mask: {debug_batch[idx]['attention_mask'][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70652f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2 - sqrt(3) + 1/(-2 - sqrt(3))\n"
     ]
    }
   ],
   "source": [
    "from sympy import symbols, Eq, solve\n",
    "\n",
    "x = symbols('x')\n",
    "eq = Eq(x**3 + 1/x**3, -52)\n",
    "\n",
    "solution = solve(eq, x)\n",
    "\n",
    "x_value = solution[0] # solution of the equation\n",
    "expr = x_value + 1/x_value\n",
    "\n",
    "print(expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19acab8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m20250827-19:00:50.282 Base HF Engine INFO: Model creation and loading time: 2.626308770850301\u001b[0m\n",
      "\u001b[37m20250827-19:00:50.390 FSDPEngine INFO: Applying FSDP2 time: 0.1044454537332058\u001b[0m\n",
      "\u001b[37m20250827-19:00:50.393 Base HF Engine INFO: Create optimizer time: 0.0011926889419555664\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `start_method` is deprecated and will be removed in a future version of wandb. This setting is currently non-functional and safely ignored.\n"
     ]
    }
   ],
   "source": [
    "rank = int(os.getenv(\"RANK\"))\n",
    "world_size = int(os.getenv(\"WORLD_SIZE\"))\n",
    "assert world_size == 1, \"This script is designed to run in a single process environment.\"\n",
    "seeding.set_random_seed(config.seed, key=f\"trainer{rank}\")\n",
    "\n",
    "worker_batch_size = config.train_dataset.batch_size\n",
    "\n",
    "actor = FSDPPPOActor(config=config.actor)\n",
    "actor.initialize(None, ft_spec)\n",
    "ref = None\n",
    "\n",
    "weight_update_meta = WeightUpdateMeta.from_disk(\n",
    "    experiment_name=config.saver.experiment_name,\n",
    "    trial_name=config.saver.trial_name,\n",
    "    file_root=config.saver.fileroot,\n",
    ")\n",
    "\n",
    "saver = Saver(config.saver, ft_spec)\n",
    "stat_logger = StatsLogger(config.stats_logger, ft_spec)\n",
    "\n",
    "total_epochs = config.total_train_epochs\n",
    "steps_per_epoch = len(dataloader)\n",
    "max_steps = total_epochs * steps_per_epoch\n",
    "start_step = config.recover_start_step or 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "022b8df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Step: 0/28439\n",
      "TensorDict(\n",
      "    fields={\n",
      "        attention_mask: Tensor(shape=torch.Size([491]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        input_ids: Tensor(shape=torch.Size([491]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        logprobs: Tensor(shape=torch.Size([491]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        loss_mask: Tensor(shape=torch.Size([491]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        rewards: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=None,\n",
      "    is_shared=False)\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "epoch = global_step // steps_per_epoch\n",
    "step = global_step % steps_per_epoch\n",
    "print(f\"Epoch {epoch}. Step: {step}/{steps_per_epoch}\")\n",
    "\n",
    "with stats_tracker.record_timing(\"rollout\"):\n",
    "    if config.async_training:\n",
    "        batch = rollout.prepare_batch(dataloader, workflow=workflow)\n",
    "    else:\n",
    "        try:\n",
    "            data = next(data_generator)\n",
    "        except StopIteration:\n",
    "            data_generator = iter(dataloader)\n",
    "            data = next(data_generator)\n",
    "        batch = rollout.rollout_batch(data, workflow=workflow)\n",
    "    \n",
    "print(batch[0])\n",
    "\n",
    "batch = batch.to(actor.device)\n",
    "\n",
    "print(len(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "142015ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491\n"
     ]
    }
   ],
   "source": [
    "print(len(batch[2][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb280c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     actor.actor.engine.eval()\n",
    "#     print(\">>> set eval mode\")\n",
    "#     assert hasattr(actor.actor.engine, \"forward\") and callable(getattr(actor.actor.engine, \"forward\")), \"actor.actor.engine does not have a callable forward method\"\n",
    "#     print(\">>> actor.actor.engine has forward\")\n",
    "#     print(\">>>\", actor.actor.engine.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5883cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if config.actor.recompute_logprob or config.actor.use_decoupled_loss:\n",
    "#     logp = actor.compute_logp(batch)\n",
    "#     batch[\"prox_logp\"] = logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24916d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for global_step in range(start_step, max_steps):\n",
    "    epoch = global_step // steps_per_epoch\n",
    "    step = global_step % steps_per_epoch\n",
    "    print(f\"Epoch {epoch}. Step: {step}/{steps_per_epoch}\")\n",
    "\n",
    "    with stats_tracker.record_timing(\"rollout\"):\n",
    "        if config.async_training:\n",
    "            batch = rollout.prepare_batch(dataloader, workflow=workflow)\n",
    "        else:\n",
    "            try:\n",
    "                data = next(data_generator)\n",
    "            except StopIteration:\n",
    "                data_generator = iter(dataloader)\n",
    "                data = next(data_generator)\n",
    "            batch = rollout.rollout_batch(data, workflow=workflow)\n",
    "\n",
    "    batch = batch.to(actor.device)\n",
    "\n",
    "    if config.actor.recompute_logprob or config.actor.use_decoupled_loss:\n",
    "        with stats_tracker.record_timing(\"recompute_logp\"):\n",
    "            logp = actor.compute_logp(batch)\n",
    "            batch[\"prox_logp\"] = logp\n",
    "            log_gpu_stats(\"recompute logp\")\n",
    "    \n",
    "    if ref is not None:\n",
    "        with stats_tracker.record_timing(\"ref_logp\"):\n",
    "            batch[\"ref_logp\"] = ref.compute_logp(batch)\n",
    "            log_gpu_stats(\"ref logp\")\n",
    "\n",
    "    with stats_tracker.record_timing(\"compute_advantage\"):\n",
    "        actor.compute_advantages(batch)\n",
    "        log_gpu_stats(\"compute advantages\")\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    with (\n",
    "        stats_tracker.record_timing(\"train_step\"),\n",
    "        stats_tracker.scope(\"grpo_actor\"),\n",
    "    ):\n",
    "        stats = actor.ppo_update(batch)\n",
    "        actor.step_lr_scheduler()\n",
    "        log_gpu_stats(\"ppo update\")\n",
    "    \n",
    "    with stats_tracker.record_timing(\"update_weights\"):\n",
    "        rollout.pause()\n",
    "        future = rollout.update_weights(weight_update_meta)\n",
    "        actor.upload_weights(weight_update_meta)\n",
    "        future.result()\n",
    "        dist.barrier(device_ids=[actor.device.index])\n",
    "        torch.cuda.synchronize()\n",
    "        rollout.resume()\n",
    "        actor.set_version(global_step + 1)\n",
    "        rollout.set_version(global_step + 1)\n",
    "    \n",
    "    stat_logger.commit(epoch, step, global_step, stats)\n",
    "\n",
    "stat_logger.close()\n",
    "rollout.destroy()\n",
    "if ref is not None:\n",
    "    ref.destroy()\n",
    "actor.destroy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
